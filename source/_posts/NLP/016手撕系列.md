---
title: 手撕softmax & MultiHeadAttention
tags: 手撕系列
categories: NLP
date: 2023-8-20 16:40:00
index_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/48.jpg
banner_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/48.jpg
math: true
---

## softmax

$$
\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

```python
import numpy as np

def softmax(x):
    """
    Compute the softmax function for each row of the input x.

    Arguments:
    x -- A N dimensional vector or M x N dimensional numpy matrix.

    Return:
    x -- You are allowed to modify x in-place
    """
    orig_shape = x.shape

    if len(x.shape) > 1:
        # Matrix
        exp_minmax = lambda x: np.exp(x - np.max(x))
        denom = lambda x: 1.0 / np.sum(x)
        x = np.apply_along_axis(exp_minmax, 1, x)
        denominator = np.apply_along_axis(denom,1,x) 
        
        if len(denominator.shape) == 1:
            denominator = denominator.reshape((denominator.shape[0],1))
        
        x = x * denominator
    else:
        # Vector
        #x_max = np.max(x)
        #x = x - x_max
        #numerator = np.exp(x)
        #denominator =  1.0 / np.sum(numerator)
        #x = numerator.dot(denominator)
    	x_max = np.max(x)
        x = x - x_max
        x = np.exp(x) / np.sum(np.exp(x))
        
    assert x.shape == orig_shape
    return x
```



> softmax的性质：$softmax(x) = softmax(x+c)$ , 利用此性质防止数值溢出
>
> https://segmentfault.com/a/1190000010039529



## MultiHeadAttention

```python
import torch.nn as nn
import torch


class MultiHeadAttention(nn.Module):

    def __init__(self, hidden_size, dropout_rate, head_size=8) -> None:
        super(MultiHeadAttention, self).__init__()
        
        self.head_size = head_size
        self.att_size = att_size = hidden_size // head_size
        
        self.scale = att_size ** -0.5

        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=False)
        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=False)
        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=False)

        self.att_dropout = nn.Dropout(dropout_rate)

        self.output_layer = nn.Linear(head_size * att_size, hidden_size, bias=False)

    def forward(self, q, k, v, mask, cache=None):
        orig_q_size = q.size()

        d_k = self.att_size
        d_v = self.att_size
        batch_size = q.size(0)

        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)

        q = q.transpose(1,2)
        v = v.transpose(1,2)
        k = k.transpose(1,2).transpose(2,3)

        q.mul_(self.scale)
        x = torch.matmul(q, k)
        x.masked_fill_(mask.unsqueeze(1), -1e9)

        x = torch.softmax(x, dim=3)
        x = self.att_dropout(x)
        x = x.matmul(v)

        x = x.transpose(1,2).contiguous()
        x = x.view(batch_size, -1, self.head_size*d_v)

        x = self.output_layer(x)

        assert x.size() == orig_q_size
        return x
```
