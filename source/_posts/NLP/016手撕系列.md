---
title: 手撕系列！！！
tags: 
categories: NLP
date: 2023-8-20 16:40:00
index_img: 
banner_img: 
math: true
---

## softmax

$$
\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

```python
import numpy as np

def softmax(x):
    """
    Compute the softmax function for each row of the input x.

    Arguments:
    x -- A N dimensional vector or M x N dimensional numpy matrix.

    Return:
    x -- You are allowed to modify x in-place
    """
    orig_shape = x.shape

    if len(x.shape) > 1:
        # Matrix
        exp_minmax = lambda x: np.exp(x - np.max(x))
        denom = lambda x: 1.0 / np.sum(x)
        x = np.apply_along_axis(exp_minmax, 1, x)
        denominator = np.apply_along_axis(denom,1,x) 
        
        if len(denominator.shape) == 1:
            denominator = denominator.reshape((denominator.shape[0],1))
        
        x = x * denominator
    else:
        # Vector
        #x_max = np.max(x)
        #x = x - x_max
        #numerator = np.exp(x)
        #denominator =  1.0 / np.sum(numerator)
        #x = numerator.dot(denominator)
    	x_max = np.max(x)
        x = x - x_max
        x = np.exp(x) / np.sum(np.exp(x))
        
    assert x.shape == orig_shape
    return x
```



> softmax的性质：$softmax(x) = softmax(x+c)$ , 利用此性质防止数值溢出
>
> https://segmentfault.com/a/1190000010039529



## MultiHeadAttention

```python
import torch.nn as nn
import torch

def initialize_weight(x):
    nn.init.xavier_uniform_(x.weight)# xavier初始化
    if x.bias is not None:
        nn.init.constant_(x.bias, 0)# 偏置初始化为0

class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size, dropout_rate, head_size=8):
        super(MultiHeadAttention, self).__init__()

        self.head_size = head_size

        self.att_size = att_size = hidden_size // head_size # d_k
        self.scale = att_size ** -0.5 # 根号下d_k分之一

        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=False) # head size降维
        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=False)
        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=False)
        initialize_weight(self.linear_q)
        initialize_weight(self.linear_k)
        initialize_weight(self.linear_v)

        self.att_dropout = nn.Dropout(dropout_rate)

        self.output_layer = nn.Linear(head_size * att_size, hidden_size,
                                      bias=False)
        initialize_weight(self.output_layer)

    def forward(self, q, k, v, mask, cache=None):
        orig_q_size = q.size() # [batch_size, max_length, embedding_dim] [1, 512, 768]

        d_k = self.att_size # 96
        d_v = self.att_size # 96
        batch_size = q.size(0) # 1

        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)
        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k) # [1, 512, 8, 96]
        if cache is not None and 'encdec_k' in cache:
            k, v = cache['encdec_k'], cache['encdec_v']
        else:
            k = self.linear_k(k).view(batch_size, -1, self.head_size, d_k)# [1, 512, 8, 96]
            v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)# [1, 512, 8, 96]

            if cache is not None:
                cache['encdec_k'], cache['encdec_v'] = k, v

        q = q.transpose(1, 2)                  # [b, h, q_len, d_k] # [1, 8, 512, 96]
        v = v.transpose(1, 2)                  # [b, h, v_len, d_v] # [1, 8, 512, 96]
        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len] # [1, 8, 96, 512]

        # Scaled Dot-Product Attention.
        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V
        q.mul_(self.scale)
        x = torch.matmul(q, k)  # [b, h, q_len, k_len] # [1, 8, 512, 96] x [1, 8, 96, 512] = [1, 8, 512, 512]
        x.masked_fill_(mask.unsqueeze(1), -1e9)# masked_fill_函数将在x中将对应于掩码为True（非零）的位置的值设为-1e9（一个很大的负值）。
        # 换句话说，它将在注意力得分中屏蔽掉掩码为True的位置
        x = torch.softmax(x, dim=3)
        x = self.att_dropout(x)
        x = x.matmul(v)  # [b, h, q_len, attn] # [1, 8, 512, 512] x [1, 8, 512, 96] = [1, 8, 512, 96]

        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn] # [1, 512, 8, 96]
        x = x.view(batch_size, -1, self.head_size * d_v) # [1, 512, 768]

        x = self.output_layer(x) # [1, 512, 768]

        assert x.size() == orig_q_size # [1, 512, 768] == [1, 512, 768]
        return x
```

