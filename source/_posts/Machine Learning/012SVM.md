---
title: SVM
tags: 
categories: Machine Learning
date: 2023-7-31 18:39:00
index_img: 
banner_img: 
math: true
---

> 一文读懂支持向量机——SVM（细节解读） - 予以初始的文章 - 知乎 https://zhuanlan.zhihu.com/p/259850768
>
> https://blog.csdn.net/qq_42791848/article/details/122328510
>
> https://blog.csdn.net/u013019431/article/details/79952483

## 定义

- 监督学习
- 分类算法

假设样本空间上有两类点，我们希望找到一个划分超平面，将这两类样本分开，而划分超平面应该选择泛化能力最好的，也就是能使得两类样本中距离它最近的样本点距离最大。

> 在特征空间中寻找间隔最大化的分离超平面的线性分类器

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-0d92783739af363b79dedfe89c16490a_1440w.webp)

两类样本点的分布如上图所示，位于中间位置的实线 H 是最优分割面，在其两侧是与之平行的两条虚线 H1、H2，两虚线之间的垂直距离就是最大化目标—间隔，位于两虚线上的样本点称为支持向量

1. 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机
2. 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机
3. 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机

即：硬间隔最大化（几何间隔）—— 学习的对偶问题——软间隔最大化（引入松弛变量）—— 非线性支持向量机（核技巧）



## 推导

超平面的数学表达：（二维空间）
$$
w^Tx + b
$$
间隔的数学表达：
$$
d=\frac{|w^Tx+b|}{||w||}
$$
d 表示**离超平面最近的样本 x (支持向量)** 到超平面的垂直距离，间隔则等于2倍的 d ，所以得到最大化目标为 2d，为了简化优化目标，省略常数项2，即最大化目标为 d

于是我们有了整个问题的目标函数：
$$
\underset{W,b}{max}\frac{|w^Tx+b|}{||w||}
$$
找到使得间隔 d 最大的 W 与 b ，即最优超平面



**约束**

求解该目标函数之前，应该遵循一个约束，就是要保证所有样本点都分类正确（硬间隔SVM）。即位于超平面上方的样本点标签为1，位于下方的样本点标签为-1
$$
\left\{\begin{array}{l}
w^T x+b \geq 0 \quad y=1 \\
w^T x+b \leq 0 \quad y=-1
\end{array}\right.
$$
该约束相当于是对 W 与 b 解空间的一个约束，相当于线性模型中加入的正则项，对权重大小进行约束，只不过是以不等式形式进行约束

**于是得到整个问题的目标函数及约束：**
$$
\underset{W,b}{max}\frac{|w^Tx+b|}{||w||}
$$

$$
s.t.\left\{\begin{array}{l}
w^T x+b \geq 0 \quad y=1 \\
w^T x+b \leq 0 \quad y=-1
\end{array}\right.
$$

**第一次化简**

硬间隔SVM要保证把所有样本点分类正确，将该约束用距离来描述：**所有样本点到超平面的距离都应大于间隔 d**，则得到约束的下述形式：
$$
\left\{\begin{array}{l}
\frac{w^T x+b}{||w||} \geq d \quad y=1 \\
\frac{w^T x+b}{||w||} \leq -d \quad y=-1
\end{array}\right.
$$
对基于距离的约束进行化简，两边同除 d 得到下式：
$$
\left\{\begin{array}{l}
\frac{w^T x+b}{||w||d} \geq 1 \quad y=1 \\
\frac{w^T x+b}{||w||d} \leq -1 \quad y=-1
\end{array}\right.
$$
因为分母为常数，所以可将其融入权重 w 与 b 中，并且不会影响最终的求解结果，得到下式：
$$
\left\{\begin{array}{l}
w^T x+b \geq 1 \quad y=1 \\
w^T x+b \leq -1 \quad y=-1
\end{array}\right.
$$
将两式合并得到约束的统一形式：
$$
y(w^Tx+b) \geq 1
$$
**（该约束对于所有的样本 x 都成立，不等式中的等号当且仅当x为支持向量时成立）**

间隔 d 是支持向量到超平面的距离，所以目标函数中的 x 表示的是支持向量，而不是全部样本点。又由约束条件可知，当 x 为支持向量时等号成立，即：
$$
y(w^Tx+b) = 1
$$
又因为 y 等于正负1，所以：
$$
|w^Tx+b|=1
$$
目标函数化简为：
$$
\underset{W,b}{max}\frac{1}{||w||}
$$
最优化通常为最小化目标，所以将目标函数进一步转换：
$$
\underset{W,b}{max}\frac{1}{||w||} = \underset{W,b}{min}||w|| = \underset{W,b}{min}\frac{1}{2}||w||^2
$$
加平方是为了方便求导，到此第一次化简就算完成了，得到的目标函数及约束如下：
$$
\underset{W,b}{min}\frac{1}{2}||w||^2
$$

$$
y_i(w^Tx_i + \gamma \geq 1) \quad \forall x_i
$$



**第二次化简**（构造同解目标函数，方便转换成对偶问题）

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801112607528.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801112646625.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801112755911.png)



**对偶问题**

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801113101638.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801113117842.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801113235988.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801113448495.png)

## 软间隔

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801114223786.png)



## 核技巧

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801114338327.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801114647491.png)



## SVM 为什么采用间隔最大化？

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强

> 几何间隔是指样本点到分类超平面的真实距离，即垂直投影到超平面上的距离。
>
> 函数间隔是指样本点与分类超平面之间的代数距离。对于超平面$wx + b = 0$和一个样本点$(x_i, y_i)$：
> $$
> 函数间隔 = y_i * (w·x_i + b)
> $$
> 几何间隔和函数间隔的关系：
> $$
> 几何间隔 = 函数间隔 / ||w||
> $$
> 在SVM中，我们希望找到具有**最大几何间隔的超平面**，这**等价于寻找具有最大函数间隔的超平面**。因此，SVM的优化目标可以表述为最大化函数间隔，并且通过求解相应的二次规划问题来找到最佳的超平面参数w和b



## 为什么 SVM 要引入核函数？

原始空间线性不可分，可以使用一个非线性映射将原始数据x 变换到另一个高维特征空间，在这个空间中，样本变得线性可分。

核函数通过计算内积而不是显式地进行特征映射，因此可以避免显式地计算高维特征空间的向量，节省计算资源。



## SVM 如何处理多分类问题？

- 一对一（One-vs-One）方法： 在一对一方法中，对于K个类别，我们构建K*(K-1)/2个二分类SVM模型。对于每两个不同的类别，我们将它们作为一个二分类问题的一部分，构建一个SVM模型。当需要对一个新样本进行分类时，每个二分类SVM都会对该样本进行预测，最后通过投票或其他策略来确定样本的最终类别
  - 优点：简单且易于实现。每个二分类SVM的训练集较小，因为它只包含两个类别的样本
  - 缺点：在某些类别不平衡或数据较多时，训练和预测时间可能会较长

- 一对其余（One-vs-Rest）方法： 在一对其余方法中，对于K个类别，我们构建K个二分类SVM模型。对于每个类别，我们将该类别的样本作为一个类别，将其他K-1个类别的样本作为另一个类别，然后构建一个SVM模型。当需要对一个新样本进行分类时，每个SVM模型都会对该样本进行预测，并选择具有最高得分的类别作为样本的最终类别
  - 优点：每个SVM模型训练集较大，使得它们在处理类别不平衡或数据较多的情况下更加稳健
  -  缺点：可能存在类别之间的冲突，导致分类不明确或不一致



## 带核的 SVM 为什么能分类非线性问题？

核函数的本质是两个函数的内积，而这个函数在 SVM 中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个内积



## 为什么要将求解 SVM 的原始问题转换为对偶问题？

- 对偶问题将原始问题中的不等式约束转为了对偶问题中的等式约束
- 方便核函数的引入
- 改变了问题的复杂度。由求特征向量 $w$转化为求比例系数$\lambda$，在原始问题下，求解的复杂度与样本的维度有关，即 $w$ 的维度。在对偶问题下，只与样本数量有关。对偶问题是凸优化问题，可以进行较好的求解



## 如何处理数据偏斜？

可以对数量多的类使得惩罚系数 C 越小表示越不重视，相反另数量少的类惩罚系数变大



## 数据分布不均匀的影响？

- 线性回归做分类因为考虑了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大
- LR 和 SVM 克服了这个缺点，其中 LR 将所有数据采用sigmod函数进行了非线性映射，使得远离分类决策面的数据作用减弱
- SVM 直接去掉了远离分类决策面的数据，只考虑支持向量的影响