---
title: KNN && K-Means
tags: 
categories: Machine Learning
date: 2023-7-31 16:21:00
index_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/2e8f0b40d620567740baafce9893b4de.png
banner_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/2e8f0b40d620567740baafce9893b4de.png
math: true
---



# KNN

> https://blog.csdn.net/weixin_45014385/article/details/123618841



## 定义

- 监督学习
- 分类算法

K近邻（K Nearest Neighbors），核心思想是在一个含未知样本的空间，可以根据样本最近的 k 个样本的数据类型来确定未知样本的数据类型

![KNN](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731183008537.png)



## 距离计算

采用欧氏距离
$$
d(x, y):=\sqrt{\left(x_1-y_1\right)^2+\left(x_2-y_2\right)^2+\cdots+\left(x_n-y_n\right)^2}=\sqrt{\sum_{i=1}^n\left(x_i-y_i\right)^2} .
$$

> [数学中的常见的距离公式](https://blog.csdn.net/jiangjiang_jian/article/details/77527855)



## K值选择

通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/9fcf1129cfef488a9e97749b75043c99.png)



## KNN特点

KNN是一种**非参的**，**惰性的**算法模型

非参的意思并不是说这个算法不需要参数，而是意味着这个模型**不会对数据做出任何的假设**，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。

惰性又是什么意思呢？同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。


## KNN优缺点

优点：

- 既可以做分类也可以做回归
- 训练速度快，时间复杂度为O(n)
- 准确率高，对数据没有假设，对离群点不敏感



缺点：

- 计算量大
- 对内存要求高，因为要储存所有的训练数据
- 存在类别不平衡问题



## 不平衡的样本可以给 KNN 的预测结果造成哪些问题，有没有什么好的解决方式？

......



## 为了解决 KNN 算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理

先将样本按距离分解成组，获得质心，然后计算未知样本到各质心的距离，选出距离最近的一组或几组，再在这些组内引用 KNN。 本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况



# K-Means

> https://blog.csdn.net/sikh_0529/article/details/126806720
>
> https://baijiahao.baidu.com/s?id=1747494468301648417&wfr=spider&for=pc



## 定义

- 无监督学习
- 聚类算法

在K-Means算法中，簇的个数K是一个超参数，需要人为输入来确定。K-Means的核心任务就是根据设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去

![K-Means](http://longls777.oss-cn-beijing.aliyuncs.com/img/2e8f0b40d620567740baafce9893b4de.png)



## K-Means 术语

- 簇：所有数据的点集合，簇中的对象是相似的
- 质心：簇中所有点的中心（计算所有点的均值而来）
- SSE：Sum of Sqared Error（误差平方和）, 它被用来评估模型的好坏，SSE 值越小，表示越接近它们的质心，聚类效果越好。由于对误差取了平方，因此更加注重那些远离中心的点（一般为边界点或离群点）



## 计算流程

1. 首先，随机确定 K 个初始点作为质心（不必是数据中的点）
2. 然后将数据集中的每个点分配到一个簇中，具体来讲，就是为每个点找到距其最近的质心，并将其分配该质心所对应的簇这一步完成之后，每个簇的质心更新为该簇所有点的平均值
3. 重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束



上述过程的 伪代码 如下:

- 创建 k 个点作为起始质心（通常是随机选择）
- 当任意一个点的簇分配结果发生改变时（不改变时算法结束）
  - 对数据集中的每个数据点
    - 对每个质心
      - 计算质心与数据点之间的距离
    - 将数据点分配到距其最近的簇
  - 对每一个簇，计算簇中所有点的均值并将均值作为质心



## K-Means 的评价标准

使用**SSE（Sum of Squared Error，误差平方和）**，其实就是**每一个点到其簇内质心的距离的平方值的总和**，这个数值对应kmeans函数中clusterAssment矩阵的第一列之和。 SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。 因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。



## K-Means算法的时间复杂度

K-Means算法是一个计算成本很大的算法。K-Means算法的平均复杂度是**O(k \* n \* T)**，其中k是超参数，即所需要输入的簇数，n是整个数据集中的样本量，T是所需要的迭代次数。在最坏的情况下，KMeans的复杂度可以写作**O(n(k+2)/p)**，其中n是整个数据集中的样本量，p是特征总数



## K-Means优缺点

优点：

- 原理比较简单，实现容易，收敛速度快
- 聚类效果较优，算法的可解释度比较强



缺点：

- K值的选取不好把握
- 对于不是凸的数据集比较难收敛
- 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳
- 采用迭代方法，得到的结果只是局部最优
- 对噪音和异常点比较敏感

