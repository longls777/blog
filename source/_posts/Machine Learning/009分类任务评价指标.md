---
title: 分类任务评价指标
tags: 
categories: Machine Learning
date: 2023-7-31 11:22:00
index_img: 
banner_img: 
math: true
---

## 混淆矩阵

|                                   | Predicted as Positive（预测-正例） | Predicted as Negative（预测-反例） |
| --------------------------------- | ---------------------------------- | ---------------------------------- |
| Labeled as Positive （真实-正例） | True Positive（TP-真正例）         | False Negative（FN-假反例）        |
| Labeled as Negative （真实-反例） | False Positive（FP-假正例）        | True Negative（TN-真反例）         |

TP：真正	【预测为1 实际为1】 

FN：假负	【预测为0 实际为1】

FP：假正	【预测为1 实际为0】

TN：真负	【预测为0 实际为0】



## 准确率 or 总体分类精度（Overall Accuracy）

分类正确的样本个数占所有样本个数的比例 
$$
OA = \frac{TP + TN}{TP + TN + FP + FN}
$$
或者
$$
OA = \frac{1}{N}\sum_{i=1}^{r}{x_{ii}}
$$


## 查准率 or 精准率（Precision）

分类正确的正样本个数占分类器分成的所有正样本个数的比例
$$
P = \frac{TP}{TP + FP}
$$


## 查全率 or 召回率（Recall）

分类正确的正样本个数占正样本个数的比例
$$
R = \frac{TP}{TP + FN}
$$




## P-R曲线

![P-R曲线](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731113655081.png)

- 一般而言，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低
- 引入“平衡点”（BEP）来度量，表示“查准率=查全率”时的取值，值越大表明分类器性能越好



## F1-Score

$$
F_{\beta} = \frac{(1+\beta^2)\times P \times R}{(\beta^2 \times P) + R}
$$

-  $\beta > 0$ 度量了查全率对查准率的相对重要性

- $\beta =1$时退化为标准的F1
- $\beta >1$时查全率有更大影响

- $\beta <1$时查准率有更大影响



F1是基于查全率和查准率的调和平均定义的：
$$
\frac{1}{F1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R})
$$
or：
$$
F1 = 2 \cdot \frac{P \cdot R}{P + R}
$$
只有在召回率 Recall 和精确率 Precision 都高的情况下，F1 score 才会很高，比 BEP 更为常用



## ROC（receiver operating characteristic curve）受试者工作特征曲线 

得此名的原因在于曲线上各点反映着相同的感受性，它们都是对同一信号刺激的反应，只不过是在几种**不同的判定标准**下所得的结果而已

> **真正例率（True Positive Rate，TPR）：**  $TPR = \frac{TP}{TP + FN}$ 也就是召回率Recall
>
> **假正例率（False Positive Rate，FPR）：** $FPR=\frac{FP}{TN + FP}$  在所有确实为“假”的样本中，被误判为真的样本所占的比例



设置不同的分类**阈值**，根据学习器的预测结果计算出真正例率、假正例率，并以真正例率为纵轴，假正例率为横轴，即得到ROC曲线

> 其实，这些点代表着一个分类器在不同阈值下的分类效果，具体的，曲线从左往右可以认为是阈值从0到1的变化过程。
>
> 当分类器阈值为0，代表不加以识别全部判断为0，此时TP=FP=0，TPR=TP/P=0，FPR=FR/N=0；当分类器阈值为1，代表不加以识别全部判断为1，此时FN=TN=0，P=TP+FN=TP, TPR=TP/P=1，N=FP+TN=FP, FPR=FR/N=1。所以，ROC曲线描述的其实是分类器性能随着分类器阈值的变化而变化的过程

![ROC曲线](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731115140261.png)

TPR 越高，同时 FPR 越低（即 ROC 曲线越陡），那么模型的性能就越好

> ROC曲线 - EricG的文章 - 知乎 https://zhuanlan.zhihu.com/p/573964757
>
> ROC曲线简介 - 李开文的文章 - 知乎 https://zhuanlan.zhihu.com/p/26293316



## AUC（Area Under Curve）

AUC是ROC曲线下的面积，AUC 的取值范围在 0.5 和 1 之间
$$
AUC = \frac{1}{2}\sum_{i=1}^{m-1}{(x_{i+1}-x_i)(y_{i+1}+y_i)}
$$
衡量二分类模型优劣的一种评价指标，一般来说，AUC 的值越大越好



#### AUC的序的定义

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731121857155.png)



#### AUC一定大于0.5吗？

 当AUC小于0.5时，只要将所有的预测概率P换成1-P，就可以让AUC大于0.5了哈哈哈哈哈

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731121749834.png)

> 你真的理解AUC么【面积与序之间的关系】 - 工藤云子的文章 - 知乎 https://zhuanlan.zhihu.com/p/360572617