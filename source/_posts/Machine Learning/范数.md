---
title: 范数
tags: 
categories: Machine Learning
date: 2022-10-13 16:05:00
index_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160652989.png
banner_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160652989.png
math: true
---

## 范数的定义

![image-20221013160652989](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160652989.png)

## 0范数

![image-20221013160736569](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160736569.png)

## 1范数

![image-20221013160753968](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160753968.png)

1范数是向量各个元素的绝对值之和，也称为曼哈顿距离

![image-20221013160924909](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160924909.png)

## 2范数

![image-20221013160945993](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013160945993.png)

2范数也就是欧氏距离

## 正无穷范数

![image-20221013161336549](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20221013161336549.png)

正无穷范数表示向量所有元素的绝对值中最大的



## 负无穷范数

负无穷范数表示向量所有元素的绝对值中最小的，证明方法类似正无穷

## 矩阵的范数

对于矩阵$A_{m×n}$， 举例而说:

```python
A = [
-1, 2, 3;
4, -6, 6;
]
```



- 矩阵的1范数（列范数）：矩阵的每一列上的元素绝对值先求和，再从中取最大值 $$ \Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}| \ \text{举例}: \Vert A\Vert_1 = max([5,8,9]) = 9 $$
- 矩阵的2范数： 矩阵 $A^TA$ 的最大特征值开平方根 $$ \Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)} $$



## 扩展——L2范数与模型泛化

使用L2范数做正则项时，可以削弱强特征，增强弱特征，这样可以避免模型过度依赖于强特征，提高模型的泛化能力，但也不是所有机器学习任务都适合使用L2正则化，在一些结构化数据挖掘任务或特征意义很模糊的机器学习任务（如深度学习）中，特征分布本来就是就是若干强特征与噪声的组合，这时加上L2范数约束反而会引入噪声，降低系统的抗噪性能，导致更差劲

建议看这篇https://mp.weixin.qq.com/s/E1Z3K3jrGLoj80ABSma6bg







> https://blog.csdn.net/qq_25847123/article/details/90400678
>
> https://github.com/songyingxin/NLPer-Interview/blob/master/1-%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0.md
>
> https://mp.weixin.qq.com/s/E1Z3K3jrGLoj80ABSma6bg