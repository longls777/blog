---
title: 深度学习中的优化算法概览
tags: 
categories: AI
date: 2023-7-19 17:52:00
index_img:
banner_img:
math: true
---

> 从 SGD 到 Adam —— 深度学习优化算法概览(一) - 骆梁宸的文章 - 知乎 https://zhuanlan.zhihu.com/p/32626442



## 什么是一阶和二阶动量？

有多种说法：

- *一阶动量和二阶动量*分别是历史梯度的一阶函数和二阶函数
- 来自chatGPT：

> 一阶动量和二阶动量是Adam优化算法中使用的两个指数移动平均项，用于估计梯度的一阶和二阶矩。
>
> 1. 一阶动量（First Moment）：一阶动量是梯度的一阶矩的估计，通常表示为m。在Adam算法中，一阶动量用于估计梯度的平均值。具体地，对于每个参数的更新，一阶动量通过指数移动平均的方式计算梯度的平均值。它可以被看作是参数梯度的历史平均，用于反映参数在更新过程中的变化趋势。
> 2. 二阶动量（Second Moment）：二阶动量是梯度平方的一阶矩的估计，通常表示为v。在Adam算法中，二阶动量用于估计梯度平方的平均值。具体地，对于每个参数的更新，二阶动量通过指数移动平均的方式计算梯度平方的平均值。它可以被看作是参数梯度平方的历史平均，用于反映参数在更新过程中的方差信息
>
> 一阶动量和二阶动量是Adam优化算法中用于估计梯度的一阶和二阶矩的指数移动平均项。它们在Adam算法中用于自适应地调整学习率和规范化参数的更新，以提高优化的性能和稳定性。

## 什么是指数移动平均？

> 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现 - Nicolas的文章 - 知乎 https://zhuanlan.zhihu.com/p/68748778

![什么是EMA](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230719180113625.png)