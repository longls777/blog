---
title: 权重初始化方法
tags: 
categories: AI
date: 2023-7-20 15:33:00
index_img: https://ask.qcloudimg.com/http-save/6430183/zv625pyxrz.png
banner_img: https://ask.qcloudimg.com/http-save/6430183/zv625pyxrz.png
math: true
---

> [网络权重初始化方法总结（上）：梯度消失、梯度爆炸与不良的初始化](https://cloud.tencent.com/developer/article/1535198)
>
> [网络权重初始化方法总结（下）：Lecun、Xavier与He Kaiming](https://cloud.tencent.com/developer/article/1542736)
>
> [一文搞懂深度网络初始化（Xavier and Kaiming initialization）](https://cloud.tencent.com/developer/article/1587082)



## 全常数初始化和全0初始化

- 全常数初始化：每一层的权重都相同，正向传播每一层的输入和输出都相同，反向传播每一层的偏导都相同，导致每层的权重向相同的方向更新，每层更新后的权重仍相同，每一层相当于一个神经元，限制了网络的表达能力
- 全0初始化：
  - 如果激活函数$g(0)=0$，那么激活函数输入输出都为0，反向传播梯度为0，权重不会更新
  - 如果$g(0)!=0$，那么权重会更新，但是每层权重的更新方向仍相同，同全常数初始化



## 过大或者过小的权重初始化

- 过大的权重初始化：如果权重初始化过大，可能导致激活函数的输出值在非线性区域接近饱和（如sigmoid函数的饱和区域），导致**梯度消失**问题。在反向传播过程中，由于梯度在饱和区域非常小，权重的更新幅度也很小，使得模型难以学习有效的特征表示，导致训练困难和性能下降。

- 过小的权重初始化：如果权重初始化过小，可能导致激活函数的输出值都接近于零，导致**梯度爆炸**问题。在反向传播过程中，梯度可能会变得非常大，导致权重参数的更新幅度过大，模型的收敛变得不稳定，甚至可能导致数值溢出。



## Lecun & Xavier & He 初始化

![](https://ask.qcloudimg.com/http-save/6430183/zv625pyxrz.png)

![image-20230720161215768](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720161215768.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720161255168.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720161334461.png)

只考虑了前向传播时，每层输出的方差保持不变，且适用于tanh等近似线性的激活函数

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720161527662.png)

同时考虑了前向和反向传播过程



对于ReLU初始化（$E(a) != 0$，a为层的输出），推导得到：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720161807743.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720162156491.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720162213255.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720162308741.png)