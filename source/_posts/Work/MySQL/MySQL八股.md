---
title: MySQL
tags: 八股
categories: MySQL
date: 2023-5-4 13:25:00
index_img: 
banner_img: 
math: true
---

# 一、事务ACID特性

**说说事务的ACID特性？**

数据库事务必须具备ACID特性，ACID是Atomic（原子性）、Consistency（一致性）、Isolation（隔离性）和Durability（持久性）

![事务](http://longls777.oss-cn-beijing.aliyuncs.com/img/事务.png)

**1. 原子性（Atomicity）**

事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。

回滚可以用回滚日志（Undo Log）来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。

**2. 一致性（Consistency）**

数据库在事务执行前后都保持一致性状态。在事务开始之前和事务结束之后，数据库的完整性约束没有被破坏。通过保证事务的原子性、隔离性和持久性，来保证事务的一致性。

**3. 隔离性（Isolation）**

一个事务在最终提交以前，对其它事务是不可见的。每个读写事务的对象对其他事务的操作对象是相互分离的。

**4. 持久性（Durability）**

一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。

系统发生崩溃可以用重做日志（Redo Log）进行恢复，从而实现持久性。与回滚日志记录数据的逻辑修改不同，重做日志记录的是数据页的物理修改。

# 二、并发一致性问题

**说说数据库并发事务会出现哪些问题？**

**丢失修改**

丢失修改指一个事务的更新操作被另外一个事务的更新操作替换。 在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 

例如：T1 和 T2 两个事务都对一个数据进行修改，T1 先修改并提交生效，T2 随后修改，T2 的修改覆盖了 T1 的修改。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207221744244.png)

**脏读**

脏读是指一个事务读取了另一个事务尚未提交的更改数据。当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。当前者回滚时，后者读到的数据是不合法的，称为脏读

例如：T1 修改一个数据但未提交，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207221920368.png)

**不可重复读**

不可重复读指在一个事务内多次读取同一数据集合。在这一事务还未结束前，另一事务也访问了该同一数据集合并做了修改，由于第二个事务的修改，第一次事务的两次读取的数据可能不一致。

例如：T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207222102010.png)

**幻读**

幻读与不可重复读类似，它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

例如：T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207222134306.png)

**幻读和不可重复读的区别在于：**

幻读对应新增（insert）数据，不可重复读对应更改（update）或者删除（delete）数据，对于不可重复读，只需要采用行级锁防止该记录数据被更改或删除，而对于幻读，则必须加表级锁，防止其他事务在这个表中新增数据。

对于前者,  只需要锁住满足条件的记录

对于后者,  要锁住满足条件及其相近的记录

**MySQL（Innodb）是如何解决幻读的？**

在一次事务里面，多次查询之后，结果集的个数不一致的情况叫做幻读。

首先我们的SELECT查询分为快照读和实时读，快照读通过MVCC（并发多版本控制）来解决幻读问题，实时读通过行锁来解决幻读问题。

因为MySQL默认的隔离级别是可重复读，这种隔离级别下，我们普通的SELECT语句都是快照读，也就是在一个事务内，多次执行SELECT语句，查询到的数据都是事务开始时那个状态的数据（这样就不会受其他事务修改数据的影响），这样就解决了幻读的问题。

> 快照读也叫一致性非锁定读，不需要等待锁的释放就可以读取，该实现通过undo log来完成，在RR和RC级别下，Innodb使用快照读，然而实现上有所不同，RR下总是读取事务开始时的行数据版本，而RC下读取最新的行数据版本，从数据库理论的角度而言，RC下的快照读违反了事务的隔离性
>
> 另外还有一致性锁定读，需要对查询语句显式地加锁
>
> 　SELECT...FOR UPDATE 
>
> 　SELECT...LOCK IN SHARE MODE
>
> SELECT...FOR UPDATE对读取的行记录加一个X锁
>
> SELECT...LOCK IN SHARE MODE对读取的行记录加一个S锁

如果说快照读总是读取事务开始时那个状态的数据，实时读就是查询总是执行这个查询时数据库中的数据。

一般使用以下这两种查询语句进行查询时就是实时读：

```sql
SELECT *** FOR UPDATE 在查询时会先申请X锁  
SELECT *** LOCK IN SHARE MODE 在查询时会先申请S锁
```

在这种情况下，我们执行第一次 SELECT...FOR UPDATE查询语句是，其实是会先申请行锁，如果一开始数据库就只有a:4一行数据，那么加锁区间其实是

(负无穷，4](4,正无穷)

我们查询条件是a>2，上面两个加锁区间都会可能有数据满足条件，所以会申请行锁中的next-key lock，是会对上面这两个区间都加锁，这样其他事务不能往这两个区间插入数据，事务B执行插入时会一直等待获取锁，直到事务A提交，释放行锁，事务B才有可能申请到锁，然后进行插入。这样就解决了幻读问题

> https://zhuanlan.zhihu.com/p/145766112

# 三、事务隔离级别

**说说事务的隔离级别？**

SQL 标准定义了四个隔离级别：

- READ-UNCOMMITTED（读取未提交）： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
- READ-COMMITTED（读取已提交）： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
- REPEATABLE-READ（可重复读）： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生（在只读时是可以避免幻读的；在读写时可能会因为update操作使得不可见的行变得可见，从而出现幻影行）
- SERIALIZABLE（可串行化）： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

| 隔离级别         | 脏读 | 不可重复读 | 幻影读 |
| ---------------- | ---- | ---------- | ------ |
| READ-UNCOMMITTED | √    | √          | √      |
| READ-COMMITTED   | ×    | √          | √      |
| REPEATABLE-READ  | ×    | ×          | √      |
| SERIALIZABLE     | ×    | ×          | ×      |

MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）

> [MySQL InnoDB RR(可重复读)隔离级别能否解决幻读](https://gaoooyh.github.io/2021-09-28-MySQL-InnoDB-RR(%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB)%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E8%83%BD%E5%90%A6%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB)

# 四、封锁

**1. 讲讲MySQL的封锁？**

**封锁粒度**

MySQL 中提供了两种封锁粒度：行级锁以及表级锁。

应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。

但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。

在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。

**封锁类型**

1. 读写锁

- 互斥锁（Exclusive），简写为 X 锁，又称写锁。
- 共享锁（Shared），简写为 S 锁，又称读锁。

有以下两个规定：

- 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。
- 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。

锁的兼容关系如下：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207213523777.png)

2. 意向锁

使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。

在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。

意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定：

- 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁；
- 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。

通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。

各种锁的兼容关系如下：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207214442687.png)

解释如下：

- 任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁；
- 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。）

**封锁协议**

**1. 三级封锁协议**

**一级封锁协议**

事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。

可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207220440451.png)

**二级封锁协议**

在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。

可以解决脏读问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207220831843.png)

**三级封锁协议**

在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。

可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/207221313819.png)

**2. 两段锁协议**

在一个事务里面，分为加锁（lock）阶段和解锁（unlock）阶段，也即所有的lock操作都在unlock操作之前

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/clipboard.png)

引入2PL是为了保证事务的隔离性，即多个事务在并发的情况下等同于串行的执行。

在实际情况下，SQL是千变万化、条数不定的，数据库很难在事务中判定什么是加锁阶段，什么是解锁阶段。于是引入了S2PL（Strict-2PL），即:

1. 在事务中只有提交（commit）或者回滚（rollback）时才是解锁阶段，
2. 其余时间为加锁阶段。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/2pl.png)

**MySQL 隐式与显示锁定**

MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。

InnoDB 也可以使用特定的语句进行显示锁定： 

```sql
SELECT ... LOCK In SHARE MODE; 
SELECT ... FOR UPDATE;
```

**2. 如何减少行锁对性能的影响？**

> https://blog.csdn.net/sinat_27143551/article/details/103033433 好文

- 根据两段锁协议，将并发加锁的语句靠近提交语句，这样可以减少加锁时间
- 死锁和死锁检测：
  - 设置死锁超时时间innodb_lock_wait_timeout 不好，时间长了等待时间长，时间短了可能回滚简单的锁等待
  - 建议开启主动死锁检测 innodb_deadlock_detect

    - 热点行死锁检测消耗大量的cpu资源，如何解决？
      - 对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作，可以通过中间件实现，或者修改MySQL源码
      - 通过设计上优化，比如将余额的1行改为10行，最后累加，不过需要仔细设计业务逻辑

如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：

- 第一种，直接执行 delete from T limit 10000;
- 第二种，在一个连接中循环执行 20 次 delete from T limit 500;
- 第三种，在 20 个连接中同时执行 delete from T limit 500。

你会选择哪一种方法呢？为什么呢？

答案：

- 方案一，事务相对较长，则占用锁的时间较长，会导致其他客户端等待资源时间较长。
- 方案二，串行化执行，将相对长的事务分成多次相对短的事务，则每次事务占用锁的时间相对较短，其他客户端在等待相应资源的时间也较短。这样的操作，同时也意味着将资源分片使用（每次执行使用不同片段的资源），可以提高并发性。
- 方案三，人为自己制造锁竞争，加剧并发量。

# 五、MCVV

**讲讲InnoDB的MVCC**

多版本并发控制（Multi-Version Concurrency Control，MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。

**基本思想**

MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。

在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。

脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。

**版本号**

- 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。
- 事务版本号 TRX_ID ：事务开始时的系统版本号。

**Undo Log**

MVCC 的多版本指的是多个版本的快照，快照存储在 Undo Log中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。

例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。

```sql
INSERT INTO t(id, x) VALUES(1, "a"); 
UPDATE t SET x="b" WHERE id=1; 
UPDATE t SET x="c" WHERE id=1;
```

因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/208164808217.png)

INSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1

**快照读与当前读**

1. 快照读

MVCC 的 SELECT 操作是针对快照中的数据，不需要进行加锁操作。

```sql
SELECT * FROM table ...;
```

2. 当前读

MVCC 会对其它对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。

```sql
INSERT; 
UPDATE; 
DELETE;
```

在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。

```sql
SELECT * FROM table WHERE ? lock in share mode; 
SELECT * FROM table WHERE ? for update;
```

# 六、B+树

**说说B+树？**

**1. 数据结构**

**B 树**

B Tree 指的是 Balance Tree，是一种自平衡的多叉树。其特点为：

- 关键字分布在整棵树中，叶子节点和非叶子节点都存放数据（即关键字也存放数据）
- 搜索有可能在非叶子节点结束
- 搜索性能等价于在关键字全集内做一次二分查找

**B+树**

B+ Tree 是基于 B Tree 和叶子结点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。其特点为：

- 关键字不保存数据，只用来索引，所有数据都保存在叶子结点
- 所有的叶子结点包含了全部关键字的信息，以及指向这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接
- 所有的非叶子结点可以看成索引部分，结点中仅含其子树中的最大或最小关键字

在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 key_i 和 key_i+1，且不为 null，则该指针指向节点的所有 key 大于等于 key_i 且小于等于 key_i+1

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/8ded5f5e7c73.png)

**B 树和 B+树区别**

- B 树的所有节点既存放 键（key）也存放 数据（data）；而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的；B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。

**2. B+树相对于B树的优势**

**（1）B+树空间利用率更高，可减少I/O次数**

​     一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。而因为B+树的内部节点只是作为索引使用，而不像B树那样每个节点都需要存储硬盘指针。

​     也就是说：B+树中每个非叶节点没有指向某个关键字具体信息的指针，所以每一个节点可以存放更多的关键字数量，即一次性读入内存所需要查找的关键字也就越多，减少了I/O操作。

​    e.g.假设磁盘中的一个盘块容纳16bytes，而一个关键字2bytes，一个关键字具体信息指针2bytes。一棵9阶B-tree（一个结点最多8个关键字）的内部结点需要2个盘块。而B+ 树内部结点只需要1个盘块。当需要把内部结点读入内存中的时候，B 树就比B+ 树多一次盘块查找时间（在磁盘中就是盘片旋转的时间）。

**（2）增删文件（节点）时，效率更高**

​     因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可以很好提高增删效率。

**（3）B+树的查询效率更加稳定**

  因为B+树的每次查询过程中，都需要遍历从根节点到叶子节点的某条路径。所有关键字的查询路径长度相同，导致每一次查询的效率相当。而Ｂ树的查找需要找到匹配元素，最好情况下查找到根节点，最坏情况下查找到叶子结点，所说性能很不稳定。

**（4）在范围查询方面，B+树更有优势**

B树的范围查找需要不断依赖中序遍历。首先二分查找到范围下限，再不断通过中序遍历，查找到范围的上限。整个过程比较耗时。

而B+树的范围查找则简单了许多。首先通过二分查找，找到范围下限，然后通过叶子结点的链表顺序遍历，直至找到上限即可，整个过程简单许多，效率也比较高。

**3. 与红黑树的比较**

红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。

（一）B+ 树有更低的树高，使得查询次数更少

平衡树的树高 O(h)=O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。

> 磁盘访问原理：
>
> 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。
>
> 如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。

（二）磁盘预读特性

为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。B+树的数据节点相互临近（因为子节点多，数据临近），能够发挥磁盘顺序读取的优势（缓存）

> http://blog.codinglabs.org/articles/theory-of-mysql-index.html

![磁盘IO过程](http://longls777.oss-cn-beijing.aliyuncs.com/img/磁盘IO过程.png)

- 第一步，首先是磁头径向移动来寻找数据所在的磁道。这部分时间叫寻道时间。
-  第二步，找到目标磁道后通过盘面旋转，将目标扇区移动到磁头的正下方。
-  第三步，向目标扇区读取或者写入数据。到此为止，一次磁盘IO完成。

故：单次磁盘IO时间 = 寻道时间 + 旋转延迟 + 存取时间。寻道耗时最长

# 七、索引

**1. 为什么要使用索引？**

- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性
- 可以大大加快 数据的检索速度（大大减少检索的数据量），这也是创建索引的最主要的原因

**2. 索引这么多优点，为什么不对表中的每一个列创建一个索引呢？（索引带来的问题）**

- 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加
- 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大
- 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度

**3. 索引的常见实现方式有哪些？**

索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现

**1. B+Tree 索引**

- 是大多数 MySQL 存储引擎的默认索引类型。
- 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。
- 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。
- 可以指定多个列作为索引列，多个索引列共同组成键。
- 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引的最左列开始查找，则无法使用索引。
- 不能跳过索引中的列。
- 如果查询中有某个列的范围查询，则其右边所有列都无法使用索引优化查找。
- InnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。

![聚簇索引](http://longls777.oss-cn-beijing.aliyuncs.com/img/262b2d6d60b9.png)

辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。

![辅助索引](http://longls777.oss-cn-beijing.aliyuncs.com/img/ec86320307ea.png)

**2. 哈希索引**

哈希索引能以 O(1) 时间进行查找，但是失去了有序性：

- 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行；
- 哈希索引不是按照索引值顺序存储的，所以无法用于排序；
- 不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的；
- 只支持等值比较查找，无法用于部分查找和范围查找；
- 哈希冲突，链表储存，哈希冲突很多，索引维护操作代价高；
- 要避免哈希冲突，必须在WHERE条件中带入哈希值和对应列值。

InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。

自定义哈希索引优化查询，如使用CRC32处理大量的URL

**3. 全文索引**

MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。

查找条件使用 MATCH AGAINST，而不是普通的 WHERE。

全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。

InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。

**4. 空间数据索引**

MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。

必须使用 GIS 相关的函数来维护数据。

**3. InnoDB怎样实现索引？**

**哈希索引**

对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择B+Tree索引。

**B+Tree索引**

MyISAM和InnoDB实现B+Tree索引方式的区别

**MyISAM**

B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录，这被称为“非聚簇索引”。

**InnoDB**

其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”，而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。

**4. B+ Tree索引和hash索引的比较**

- 在等值查询中，哈希索引具有明显优势，因为只需要一次hash算法就可以找到相应键值，当然，前提是键值都是唯一的，否则就需要考虑hash冲突问题
- 在范围查询中，就需要采用B+树索引，使用hash索引进行范围查询非常浪费资源，每一个数据都需要一次hash查询，而B+树是有序的，在这种范围查询中就优势非常大了，同理，也无法利用hash索引排序

**5.覆盖索引**

**什么是覆盖索引**

如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢，覆盖索引就是要查询出的列和索引是对应的，不做回表操作。

**覆盖索引使用实例**

现在我创建了索引(username, age)，我们执行下面的 sql 语句

```sql
select username, age from user where username = 'Java' and age = 22
```

在查询数据的时候：要查询出的列在叶子节点都存在！所以，就不用回表。

**6.非主键索引的查询一定会回表吗？**

不一定，当查询语句的要求字段全部命中索引，不用回表查询。如select 主键 from 非主键=XX，此时非主键索引叶子节点即可拿到主键信息，不用回表。

**7.哪些列适合作索引，哪些不适合？**

**适合作为索引：**

- 在经常需要搜索的列上，可以加快搜索的速度；
- 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度；
- 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；
- 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度；

**不适合作为索引：**

- 那些在查询中很少使用的列；
- 那些只有很少数据值的列，比如性别，查询结果的数据行占了表中数据行的很大比例，需要在表中搜索的数据行很多，并不能加快检索速度；
- 对于text，image，bit类型的列不应该增加索引，因为这些列的数据量要么相当大，要么取值很少；

**8.什么是最左前缀匹配原则？**

- **最左前缀匹配原则**：mysql会一直向右匹配直到遇到范围查询（>, <, between, like） 就会停止匹配，比如 a = 3 and b = 4 and c > 5 and d = 6 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
- **=和in可以乱序**，比如 a = 1 and b = 2 and c = 3 ，建立 (a,b,c) 的索引可以任意顺序，mysql优化器会帮你优化成索引可以识别的形式
- 从本质上来说，联合索引还是一颗B+树，不同的是联合索引的键值的数量不是1，而是大于等于2。假设一个表有a，b两个字段，且建立了(a,b)联合索引，该索引的B+ 树如下图所示：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/d98410803b5a.png)

键值都是排序的，通过叶节点可以逻辑上顺序地读出所有数据，就上面的例子来说即：（1, 1），（1, 2），（2, 1），（2, 4），（3, 1），（3, 2）。数据按（a, b）的顺序进行了存放。

因此，对于查询 SELECT * FROM TABLE WHERE a=xxx and b=xxx ，显然是可以使用（a,b）的这个联合索引。对于单个的a列查询 SELECT * FROM TABLE WHERE a=xxx 也是可以使用这个（a,b）索引。但是对于b列的查询 SELECT * FROM TABLE WHERE b=xxx ，不可以使用这颗B+树索引。可以看到叶节点上的b值为1、2、1、4、1、2，显然不是排序的，因此对于b列的查询使用不到（a,b）的索引。

# 八、undo log

**说说什么是undo log？**

- undo log就是回滚日志，其储存在数据库内部的undo segment中，undo 段位于共享表空间

- undo log是逻辑日志，回滚时，所有的修改会被逻辑的取消，但是数据结构和页本身在回滚后可能大不相同，这样可以避免在并发事务环境下对其他事务造成影响，如表空间因为insert增大后，回滚不会还原表空间的大小

- undo log的另一个作用是MVCC

- undo log会产生redo log，即事务在undo log segment分配页并写入undo log这个过程同样需要写入重做日志

- 从1.1版本开始，undo段中最多有128个rollback segment，每个rollback segment记录1024个undo log segment，所以支持同时在线128*1024个事务

- 事务提交后并不会马上删除undo log，因为可能还有其他事务通过undo log读取行记录之前的版本，事务提交后会将undo log放入一个链表中，由purge线程判断是否删除

- undo log分为两种

- - insert undo log       事务提交后可以直接删除
  - update undo log	需要提供MVCC机制，因此不能直接删除

- 通过对undo log的分析，发现delete和update都不会立即删除原行记录，而是标记为删除，由purge线程判断是否需要提供MVCC，如果不需要，再由purge线程删除

# 九、redo log和bin log

> https://www.yuque.com/huhuitao-sssvf/gg0865/yap9bq 必看

**1. 说说什么是redo log？**

redo log 是 InnoDB 存储引擎所特有的，InnoDB 存储引擎由内存池和一些后台线程组成：

![Innodb引擎](http://longls777.oss-cn-beijing.aliyuncs.com/img/Innodb引擎.png)

**内存池**

先来解释下内存池。

首先，我们需要知道，InnoDB 存储引擎是基于磁盘存储的，并将其中的记录按照页的方式进行管理。因此可将其视为基于磁盘的数据库系统（Disk-base Database），在这样的系统中，众所周知，由于 CPU 速度与磁盘速度之间的不匹配，通常会使用缓冲池技术来提高数据库的整体性能。

所以这里的内存池也被称为缓冲池（简单理解为缓存就好了）。

具体来说，缓冲池其实就是一块内存区域，在 CPU 与磁盘之间加入内存访问，通过内存的速度来弥补磁盘速度较慢对数据库性能的影响。

拥有了缓冲池后，“读取页” 操作的具体步骤就是这样的：

- 首先将从磁盘读到的页存放在缓冲池中
- 下一次再读相同的页时，首先判断该页是否在缓冲池中。若在缓冲池中，称该页在缓冲池中被命中，直接读取该页。否则，读取磁盘上的页。

“修改页” 操作的具体步骤就是这样的：

首先修改在缓冲池中的页；然后再以一定的频率刷新到磁盘上。

所谓 ”脏页“ 就发生在修改这个操作中，如果缓冲池中的页已经被修改了，但是还没有刷新到磁盘上，那么我们就称缓冲池中的这页是 ”脏页“，即缓冲池中的页的版本要比磁盘的新。

至此，综上所述，我们可以得出这样的结论：缓冲池的大小直接影响着数据库的整体性能。

**后台线程**

后台线程其实最大的作用就是用来完成 “将从磁盘读到的页存放在缓冲池中” 以及 “将缓冲池中的数据以一定的频率刷新到磁盘上” 这俩个操作的，当然了，还有其他的作用。以下是《MySQL 技术内幕：InnoDB 存储引擎 - 第 2 版》对于后台线程的描述：

后台线程的主要作用就是刷新内存池中的数据，保证内存池中缓存的是最近的数据；此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下 InnoDB 能恢复到正常运行状态。

另外，InnoDB 存储引擎是多线程的模型，也就是说它拥有多个不同的后台线程，负责处理不同的任务。这里简单列举下几种不同的后台线程：

- Master Thread：主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性
- IO Thread：在 InnoDB 存储引擎中大量使用了 AIO（Async IO）来处理写 IO 请求，这样可以极大提高数据库的性能。IO Thread 的工作主要是负责这些 IO 请求的回调（call back）处理
- Purge Thread：回收已经使用并分配的 undo 页
- Page Cleaner Thread：将之前版本中脏页的刷新操作都放入到单独的线程中来完成。其目的是为了减轻原 Master Thread 的工作及对于用户查询线程的阻塞，进一步提高 InnoDB 存储引擎的性能

**redo log 与 WAL 策略**

上文我们提到，当缓冲池中的某页数据被修改后，该页就被标记为 ”脏页“，脏页的数据会被定期刷新到磁盘上。

倘若每次一个页发生变化，就将新页的版本刷新到磁盘，那么这个开销是非常大的。并且，如果热点数据都集中在某几个页中，那么数据库的性能将变得非常差。另外，如果在从缓冲池将页的新版本刷新到磁盘时发生了宕机，那么这个数据就不能恢复了。

所以，为了避免发生数据丢失的问题，当前事务数据库系统（并非 MySQL 所独有）普遍都采用了 WAL（Write Ahead Log，预写日志）策略：即当事务提交时，先写重做日志（redo log），再修改页（先修改缓冲池，再刷新到磁盘）；当由于发生宕机而导致数据丢失时，通过 redo log 来完成数据的恢复。这也是事务 ACID 中 D（Durability 持久性）的要求。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

每个 InnoDB 存储引擎至少有 1 个重做日志文件组（redo log group），每个文件组下至少有 2 个重做日志文件（redo log file），默认的话是一个 redo log group，其中包含 2 个 redo log file：ib_logfile0 和 ib_logfile1 。

一般来说，为了得到更高的可靠性，用户可以设置多个镜像日志组（mirrored log groups），将不同的文件组放在不同的磁盘上，以此提高 redo log 的高可用性。在日志组中每个 redo log file 的大小一致，并以循环写入的方式运行。

如下图，一个 redo log group，包含 3 个 redo log file：

![redo log group](http://longls777.oss-cn-beijing.aliyuncs.com/img/redo_log_group.png)

InnoDB 存储引擎会先写 redo log file 0，当 file 0 被写满的时候，会切换至 redo log file 1，当 file 1 也被写满时，会切换到 redo log file 2 中，而当 file 2 也被写满时，会再切换到 file 0 中。

可以看出，redo log file 的大小设置对于 InnoDB 存储引擎的性能有着非常大的影响：

- redo log file 不能设置得太大，如果设置得很大，再恢复时可能需要很长的时间
- redo log file 又不能设置得太小了，否则可能导致一个事务的日志需要多次切换重做日志文件

**CheckPoint 技术**

有了 redo log 就可以高枕无忧了吗?显然不是这么简单，我们仍然面临这样 3 个问题：

1. 缓冲池不是无限大的，也就是说不能没完没了的存储我们的数据等待一起刷新到磁盘

2. redo log 是循环使用而不是无限大的（也许可以，但是成本太高，同时不便于运维），那么当所有的 redo log file 都写满了怎么办？

3. 当数据库运行了几个月甚至几年时，这时如果发生宕机，重新应用 redo log 的时间会非常久，此时恢复的代价将会非常大。

因此 Checkpoint 技术的目的就是解决上述问题：

- 缓冲池不够用时，将脏页刷新到磁盘
- redo log 不可用时，将脏页刷新到磁盘
- 缩短数据库的恢复时间

所谓 CheckPoint 技术简单来说其实就是在 redo log file 中找到一个位置，将这个位置前的页都刷新到磁盘中去，这个位置就称为 CheckPoint（检查点）。

针对上面这三点我们依次来解释下：

1. 缩短数据库的恢复时间：当数据库发生宕机时，数据库不需要重做所有的日志，因为 Checkpoint 之前的页都已经刷新回磁盘。故数据库只需对 Checkpoint 后的 redo log 进行恢复就行了。这显然大大缩短了恢复的时间。

2. 缓冲池不够用时，将脏页刷新到磁盘：所谓缓冲池不够用的意思就是缓冲池的空间无法存放新读取到的页，这个时候 InnoDB 引擎会怎么办呢？LRU 算法。InnoDB 存储引擎对传统的 LRU 算法做了一些优化，用其来管理缓冲池这块空间。

> 总的思路还是传统 LRU 那套，具体的优化细节这里就不再赘述了：即最频繁使用的页在 LRU 列表（LRU List）的前端，最少使用的页在 LRU 列表的尾端；当缓冲池的空间无法存放新读取到的页时，将首先释放 LRU 列表中尾端的页。这个被释放出来（溢出）的页，如果是脏页，那么就需要强制执行 CheckPoint，将脏页刷新到磁盘中去。

3. redo log 不可用时，将脏页刷新到磁盘：

所谓 redo log 不可用就是所有的 redo log file 都写满了。但事实上，其实 redo log 中的数据并不是时时刻刻都是有用的，那些已经不再需要的部分就称为 ”可以被重用的部分“，即当数据库发生宕机时，数据库恢复操作不需要这部分的 redo log，因此这部分就可以被覆盖重用（或者说被擦除）。

举个例子来具体解释下：一组 4 个文件，每个文件的大小是 1GB，那么总共就有 4GB 的 redo log file 空间。write pos 是当前 redo log 记录的位置，随着不断地写入磁盘，write pos 也不断地往后移，就像我们上文说的，写到 file 3 末尾后就回到 file 0 开头。CheckPoint 是当前要擦除的位置（将 Checkpoint 之前的页刷新回磁盘），也是往后推移并且循环的：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/fb3d4b95ec5.jpeg)

write pos 和 CheckPoint 之间的就是 redo log file 上还空着的部分，可以用来记录新的操作。如果 write pos 追上 CheckPoint，就表示 redo log file 满了，这时候不能再执行新的更新，得停下来先覆盖（擦掉）一些 redo log，把 CheckPoint 推进一下。

综上所述，Checkpoint 所做的事情无外乎是将缓冲池中的脏页刷新到磁盘。不同之处在于每次刷新多少页到磁盘，每次从哪里取脏页，以及什么时间触发 Checkpoint。在 InnoDB 存储引擎内部，有两种 Checkpoint，分别为：

- Sharp Checkpoint：发生在数据库关闭时将所有的脏页都刷新回磁盘，这是默认的工作方式，参数 innodb_fast_shutdown=1
- Fuzzy Checkpoin：InnoDB 存储引擎内部使用这种模式，只刷新一部分脏页，而不是刷新所有的脏页回磁盘。关于 Fuzzy CheckPoint 具体的情况这里就不再赘述了

> 好文 https://database.51cto.com/art/202106/669168.htm

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/mlxcccccccc.png)

**2.bin log和redo log的区别？**

**bin log (binary log)**

- Mysql Server层面的
- 记录了对 MySQL 数据库执行更改的所有操作，但是不包括 SELECT 和 SHOW 这类操作，因为这类操作对数据本身并没有修改
- 逻辑日志，记录的是这个语句的原始逻辑，比如 “给 ID=1 这一行的 a 字段加 1”
- 追加写，一份日志文件写到一定大小的时候会更换下一个文件，不会覆盖
- bin log仅在事务提交前写入，只写磁盘一次
- bin log用来做数据归档，但不具备崩溃恢复的能力
- 默认bin log不是每次写的时候都同步磁盘（缓冲写），当系统宕机时，可能会有一部分数据没写入binlog中。参数sync_binlog = N 表示写入缓冲多少次就同步到磁盘。N=1表示每次事务的binlog都持久化到磁盘，不会用到操作系统的缓冲

**redo log（重做日志）**

- Innodb引擎层面的

- 物理日志，记录的是物理数据页面的修改的信息

- 循环写，会覆盖之前的日志文件，当然，被覆盖的日志文件对应的脏页已经被写入磁盘了，覆盖了也没有影响

- redo log在事务进行过程中会不停的写入

- redo log具有crash-safe能力

- 写入redo log不是直接写磁盘，而是先写入一个重做日志缓冲（redo log buffer），然后按一定的条件写入

- - innodb_flush_log_at_trx_commit参数：为了保证事务的持久性，应该设为1

  - - 0：表示事务提交的时候，不主动将事务的重做日志写入磁盘，而是等待主线程刷新
    - 1：事务提交，将重做日志刷入磁盘，并伴有fsync调用
    - 2：将重做日志异步写入磁盘，即写入到文件系统的缓存中。因此并不能完全保重写入到磁盘

**3.为什么有了bin log还要有redo log？**

1. InnoDB在作为MySQL的插件加入MySQL引擎家族之前，就已经是一个提供了崩溃恢复和事务支持的引擎了。InnoDB接入了MySQL后，发现既然binlog没有崩溃恢复的能力，那引入InnoDB原有的redo log来保证崩溃恢复能力。 
2. binlog没有记录数据页修改的详细信息，不具备恢复数据页的能力。binlog记录着数据行的增删改，但是不记录事务对数据页的改动，这样细致的改动只记录在redo log中。当一个事务做增删改时，其实涉及到的数据页改动非常细致和复杂，包括行的字段改动以及行头部以及数据页头部的改动，甚至b+tree会因为插入一行而发生若干次页面分裂，那么事务也会把所有这些改动记录下来到redo log中。因为数据库系统进程crash时，磁盘上面页面镜像可以非常混乱，其中有些页面含有一些正在运行着的事务的改动，而一些已提交的事务的改动并没有刷上磁盘。事务恢复过程可以理解为是要把没有提交的事务的页面改动都去掉，并把已经提交的事务的页面改动都加上去这样一个过程。这些信息，都是binlog中没有记录的，只记录在了存储引擎的redo log中。
3. 操作写入binlog可细分为write和fsync两个过程，write指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，fsync才是将数据持久化到磁盘的操作。通过参数设置sync_binlog为0的时候，表示每次提交事务都只write，不fsync。此时数据库崩溃可能导致部分提交的事务以及binlog日志由于没有持久化而丢失。

**3. redo log的写入机制？**

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/redo_log.png)

为了控制 redo log 的写入策略， InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：

1. 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;

2. 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘（调用fsync操作）；

3. 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache 。

同时，innodb中有一个后台线程，每隔一秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的page cache ，然后调用 fsync 持久化到磁盘。这就是后台线程的轮询操作

实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。

1. 一种是， redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。这个写盘动作只是 write ，而没有调用 fsync ，也就是只留在了文件系统的 page cache中

2. 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1 ，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redolog buffer 里的日志一起持久化到磁盘。

为了确保每次日志都能写入到事务日志文件中，在每次将log buffer中的日志写入日志文件的过程中都会调用一次操作系统的fsync操作（即fsync系统调用）。因为MySQL是工作在用户空间的，MySQL的log buffer处于用户空间的内存中。要写入到磁盘上的log file中，中间还要经过操作系统内核空间的os buffer，调用fsync的作用就是将OS buffer中的日志刷到磁盘上的log file中

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/buffer.png)

**4. bin log的写入机制？**

区分于redo log的事务执行过程中会不断的写入，bin log 是在事务最终提交前写入的

具体的实现方法就是：事务执行过程中，先把日志写到 bin log cache ，事务提交的时候，再把 bin log cache 写到 bin log 文件中

一个事务的 bin log 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入，这就涉及到了bin log cache保存这个事务bin log 的能力

系统给 bin log cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 bin log cache 所占内存的大小

如果超过了这个参数规定的大小，就要暂存到磁盘，事务提交的时候，把 binlog cache 里的完整事务写入到 bin log 中，并清空 bin log cache

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/jajdfa.png)

每个线程有自己 bin log cache（binlog cache 是每个线程自己维护的），但是共用同一份 bin log 文件，保证了一个事务的bin log不能被拆开。bin log的阶段就三种：

图中的 write ，指的就是指把日志写入到文件系统的 page cache（是操作系统的文件系统上的，属于操作系统的内存），并没有把数据持久化到磁盘，所以速度比较快。

图中的 fsync ，才是将数据持久化到磁盘的操作（由系统缓冲区写回磁盘的函数）。一般情况下，我们认为 fsync 才占磁盘的IOPS（IOPS （Input/Output Operations Per Second），即每秒进行读写（I/O）操作的次数）

write  和 fsync 的时机，是由参数 sync_binlog 控制的：

1. sync_binlog=0 的时候，表示每次提交事务都只 write ，不 fsync （不建议这样设置）

2. sync_binlog=1 的时候，表示每次提交事务都会执行 fsync ；

3. sync_binlog=N(N>1) 的时候，表示每次提交事务都 write ，但累积 N 个事务后才 fsync 

所以在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。但是，如果主机发生异常重启，会丢失最近 N 个事务的binlog 日志，因为一直没有fsync

通常我们说 MySQL 的 “ 双 1” 配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1 。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（且此时redo log处于prepare阶段），一次是 binlog 。

**5. 什么是两阶段提交？**

为了保证bin log和redo log两份日志的逻辑一致，最终保证恢复到主备数据库的数据是一致的，采用两阶段提交的机制。

1. 执行器调用存储引擎接口，存储引擎将修改更新到内存中后，将修改操作记录redo log中，此时redo log处于prepare状态。
2. 存储引擎告知执行器执行完毕，执行器生成这个操作对应的binlog，并把bin log写入磁盘。
3. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交commit状态，更新完成。

- perpare阶段
  - 写入redo log
  - 设置undo state=TRX_UNDO_PREPARED;
  - 刷新事务更新产生的redo log
- commit阶段
  - 写入bin log
  - 将事务产生的bin log写入文件，写到磁盘
  - 设置undo页的状态，记录事务对应的bin log 偏移写入系统表空间。

两阶段提交是跨系统维持数据逻辑一致性的常用方案。两阶段存在阻塞，于是在两阶段提交的基础上增加了预提交

# 十、索引优化

**1. 独立的列**

 如果查询中的列不是独立的，那么MySQL就不会使用索引

```sql
select actor_id from sakila.actor where actor_id + 1 = 5;
```

**2. 前缀索引和索引选择性**

- 有时候需要索引很长的字符列，这会让索引变得大且慢。通常可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。索引的选择性是指不重复的索引值（也称为基数，cardinality）和数据表的记录总数的比值，范围从1/#T 到 1之间。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。
- 对于BLOB，TEXT，或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。
- 诀窍在于要选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）。前缀应该足够长，以使得前缀索引的选择性接近于索引的整个列。
- 前缀索引是一种能使索引更小，更快的有效办法，但另一方面也有其缺点：
  - mysql无法使用其前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描

# 十一、三大范式

**第一范式（1NF）**

所谓第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。

理解：列不可分

**第二范式（2NF）**

第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被唯一的区分。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识。要求实体的属性完全依赖于主关键字。

理解：不能部分依赖，即一张表存在组合主键时，其他非主键字段不能部分依赖

**第三范式（3NF）**

 满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。

在第二范式的基础上，数据表中如果不存在非关键字段对任一候选关键字段的传递函数依赖则符合第三范式。

理解：不能存在传递依赖，即除主键外，其他字段必须依赖主键

# 十二、Mysql语句执行过程

**1. 一条Mysql语句的执行过程？**

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/snacdjk.png)

1. 客户端首先通过连接器进行身份认证和权限相关
2. 如果是执行查询语句的时候，会先查询缓存，但MySQL 8.0 版本后该步骤移除
3. 没有命中缓存的话，SQL 语句就会经过解析器，分析语句，包括语法检查等等
4. 通过优化器，将用户的SQL语句按照 MySQL 认为最优的方案去执行
5. 执行语句，并从存储引擎返回数据

- Mysql

- - Server层

  - - 实现跨储存引擎功能：储存过程、触发器、视图
    - 连接器
    - 查询缓存
    - 分析器
    - 优化器
    - 执行器

  - 存储引擎层

  - - 负责数据的存储和提取
    - 插件式

**连接器**

首先连接数据库，连接器负责跟客户端建立连接，获取权限，维持和管理连接

```bash
mysql -h ip -P port -u user -p password
```

输入完命令之后，需要在交互会话中输入密码，虽然密码也可以直接在-p参数后面但是这样会导致密码泄露。如果连接的生产服务器，建议不要这么做。

连接命令中的mysql是客户端工具，用来跟服务器端建立连接。完成TCP握手后，连接器开始认证你的身份，这时候就需要输入的用户名和密码。

如果用户名，密码不对，就会收到“Access denied for user”的错误，然后客户端程序结束执行。

如果用户名密码认证通过，连接器会到权限表里面查到你拥有的权限。在这个链接里面的权限判断逻辑，都依赖与此时读到的权限。

这就意味着用户成功建立连接后，即使你现在用管理员账号对这个用户的权限做了修改，也不会影响已经存在的连接的权限。修改完成后，重新建立连接时权限才生效。

> MySQL中存在4个控制权限的表，分别为user表，db表，tables_priv表，columns_priv表
>
> mysql权限表的验证过程为：
>
> 先从user表中的Host，User，Password这3个字段中判断连接的ip、用户名、密码是否存在，存在则通过验证。
>
> 通过身份认证后，进行权限分配，按照user，db，tables_priv，columns_priv的顺序进行验证。即先检查全局权限表user，如果user中对应的权限为Y，则此用户对所有数据库的权限都为Y，将不再检查db，tables_priv，columns_priv；如果为N，则到db表中检查此用户对应的具体数据库，并得到db中为Y的权限；如果db中为N，则检查tables_priv中此数据库对应的具体表，取得表中的权限Y，以此类推。
>
> 参见https://www.cnblogs.com/keme/p/10288168.html

**查缓存**

连接建立成功之后，可以执行select语句。执行逻辑就会来到第二步：查缓存

MySQL接收到一个查询请求时，会先到缓存中看看，之前是不是已经执行过这条语句。之前执行过的语句以及结果会以key-value的形式被缓存在内存中。key是查询语句，value是查询结果。如果你的查询能够直接在这个缓存中找到key，那么这个value会被直接返回客户端。

如果不在缓存中，就会继续执行后面的阶段。执行完后会被存入查询缓存中。如果缓存命中，MySQL不需要执行后面的复杂操作就可以直接返回，但是大多数情况下缓存失效的情况会很多。只要对表更新一次，这个表中的所有查询缓存都会被清空。除非你的业务就是一张静态表，只读不写。例如系统配置表。

可以不使用queryCache，query_cache_type=DEMAND，这样对于默认的SQL语句都不使用查询缓存。如果想使用缓存可以显示指定。例如：

```sql
select SQL_CACHE * from T where ID=18;
```

**分析器**

没使用查询缓存时，就开始真正执行语句了，首先MySQL对语句进行解析。

分析器会使用词法分析。如果的多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。

MySQL从你输入的select这个关键字识别出来，这是一个查询语句。

语法分析：根据词法分析得出的结果，语法分析会根据语法规则判断输入的SQL语句是否满足MySQL的语法。

如果语法不对会出现“You have an error in your SQL syntax”提示。

**优化器**

经过分析器MySQL就知道你想要做什么，再开始执行之前，还要先经过优化器的处理。优化器是在表里面有多个索引的时候，决定使用哪个索引，或者在一个语句中有多个表关联的时候，决定各个表的连接顺序。

优化器完成优化后，这个语句的执行方案就确定下来了，然后开始进行执行阶段。

**执行器**

**开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限**，如果没有，就会返回没有权限的错误。

如果有权限，就会打开表继续执行，执行器会根据表选择的存储引擎，去使用引擎提供的接口。

例如：select * from T where ID=10;

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行结果加入到结果集。
2. 继续判断下一行记录，重复上一步的逻辑判断，直到取出最后一行。	
3. 将结果集返回给客户端。

**2. update语句执行流程？**

1. 执行器先找引擎取出ID=2这行数据。引擎直接用树搜索找到这一行。如果ID=2这行所在的数据页本来就在内存中，就直接返回给执行器；否则需要从磁盘中读取载入内存。
2. 执行器拿到引擎给的行数据，将这个值加1，得到一行新数据，在调用引擎接口写入这行数据。
3. 引擎将这行新数据更新到内存中，同时将这行更新操作记录到redo log中，此时redo log处于prepare状态，然后告知执行器已经执行完成，随时可以提交事务。
4. 执行器生成这个操作的bin log，将bin log写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎将写入的redo log改成提交状态，更新完成。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/dajfksjf.png)

浅绿色：InnoDB执行，深绿色：执行器执行

# 十三、MyISAM和InnoDB

**MyISAM和InnoDB的区别？**

-  InnoDB支持事务，MyISAM不支持，对于InnoDB每一条SQL语言都默认封装成事务，自动提交；
- InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MyISAM会失败； 
- InnoDB是聚簇索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高；MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。InnoDB主键索引和辅助索引是独立的。也就是说：InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针。
- InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快（注意不能加任何WHERE条件）；
- Innodb不支持全文索引，而MyISAM支持全文索引，在涉及全文索引领域的查询效率上MyISAM速度更快；PS：5.7以后的InnoDB支持全文索引了；
- InnoDB支持表、行（默认）级锁，而MyISAM只支持表级锁；
- InnoDB表必须有唯一索引（如主键）（用户没有指定的话会自己找/生产一个隐藏列Row_id来充当默认主键），而MyISAM可以没有

# 十四、内连接和外连接

**内连接与外连接**

- 对于内连接的两个表，驱动表中的记录在被驱动表中找不到匹配的记录，该记录不会加入到最后的结果集。
- 对于外连接的两个表，驱动表中的记录即使在被驱动表中没有匹配的记录， 也仍然需要加入到结果集。

在 MySQL 中，根据选取驱动表的不同，外连接仍然可以细分为 2 种：

- **左外连接**，选取左侧的表为驱动表。
- **右外连接**，选取右侧的表为驱动表。

**外连接**

即使对于外连接来说，有时候我们也并不想把驱动表的全部记录都加入到最后的结果集。怎么办？把过滤条件分为两种就可以就解决这个问题了，所以放在不同地方的过滤条件是有不同语义的：

**WHERE 子句中的过滤条件**

不论是内连接还是外连接，凡是**不符合 WHERE 子句中的过滤条件的记录都不会被加入最后的结果集**

**ON 子句中的过滤条件**

对于**外连接**的驱动表的记录来说，如果无法在被驱动表中找到匹配 ON 子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用 NULL 值填充。

需要注意的是，这个 ON 子句是专门为外连接驱动表中的记录在被驱动表找不到匹配记录时应不应该把该记录加入结果集这个场景下提出的，所以如果把 ON 子句放到内连接中，MySQL 会把它和 WHERE 子句一样对待，也就是说：**内连接中的 WHERE 子句和 ON 子句是等价的**。

一般情况下，我们都把只涉及单表的过滤条件放到 WHERE 子句中，把涉及两表的过滤条件都放到 ON 子句中，我们也一般把放到 ON 子句中的过滤条件也称之为连接条件。

**内连接**

连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。不论哪个表作为驱动表，两表连接产生的笛卡尔积肯定是一样的。而对于内连接来说，由于凡是不符合 ON 子句或 WHERE 子句 中的条件的记录都会被过滤掉，其实也就相当于从两表连接的笛卡尔积中把不符合过滤条件的记录给踢出去，所以对于内连接来说，驱动表和被驱动表是可以互换的，并不会影响最后的查询结果。

但是对于外连接来说，由于驱动表中的记录即使在被驱动表中找不到符合 ON 子句条件的记录时也要将其加入到结果集，所以此时驱动表和被驱动表的关系就很重要了，也就是说左外连接和右外连接的驱动表和被驱动表不能轻易互换。

# 十四、主键

**Mysql建表时不设置主键会怎样？**

在mysql的技术文档里面有如下文字：

​     If you do not define a PRIMARY KEY for your table, MySQL picks the first UNIQUE index that has only NOT NULL columns as the primary key and InnoDB uses it as the clustered index. If there is no such index in the table, InnoDB internally generates a clustered index where the rows are ordered by the row ID that InnoDB assigns to the rows in such a table. The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in insertion order. 

翻译一下就是：如果没有主动设置主键，就会选一个不包含NULL的第一个唯一索引列作为主键列，并把它用作一个聚集索引。如果没有这样的索引就会使用行号生成一个聚集索引，把它当做主键，这个行号 6 bytes，自增。可以用select _rowid from table来查询

# 十六、varchar char text

**1. varchar和char有什么区别？**

- char定长，插入长度不足定义长度时，使用空格填充，取出时去除末尾空格（包括原有的）；varchar变长，插入多少字符就存储多少字符
- char理论最大长度为255字节，varchar理论最大长度为65535字节，最长字符数则要考虑编码方式。（varchar有1或2个字节来保存长度信息，字符串长度超过255则使用2个字节保存）
- char的处理速度比varchar快，因为是定长
- char比varchar更耗费空间，因为要做填充

> 当所插入的字符串超出它们的长度时，视情况来处理，如果是严格模式，则会拒绝插入并提示错误信息，如果是宽松模式，则会截取然后插入。
>
> Mysql 5中
>
> 非空CHAR的最大总长度是255【字节】；非空VARCHAR的最大总长度是65533【字节】
>
> 可空CHAR的最大总长度是254【字节】；可空VARCHAR的最大总长度是65532【字节】
>
> 原因：为空标记需要占据一个字节，VARCHAR超过255需要用2个字节标记字段长度，不超过255用1个字节标记字段长度
>
> 根据官方文档，MySQL一行最多存储65535个字节

**2. 如何选择varchar还是char？**

当我们为字符串类型的字段选取类型的时候，判断该选取VARCHAR还是CHAR，我们可以从以下几个方面来考虑：

- 该字段数据集的平均长度与最大长度是否相差很小，若相差很小优先考虑CHAR类型，反之，考虑VARCHAR类型。
- 若字段存储的是MD5后的哈希值，或一些定长的值，优先选取CHAR类型。
- 若字段经常需要更新，则优先考虑CHAR类型，由于CHAR类型为定长，因此不容易产生碎片。
- 对于字段值存储很小的信息，如性别等，优先选取CHAR类型，因为VARCHAR类型会占用额外的字节保存字符串长度信息。

总之一句话，当我们能够选取CHAR类型的时候，或者说空间消耗相对并不是影响因素的重点时，尽量选取CHAR类型，因为在其他方面，CHAR类型都有着或多或少的优势。而当空间消耗成为了很大的影响因素以后，我们则考虑使用VARCHAR类型。

**3. varchar和text的区别？**

- text不允许有默认值，建立索引必须给出前缀索引长度；varchar允许有默认值
- 可以考虑用varchar替代text，因为varchar存储更弹性，存储数据少的话性能更高
- 如果存储的数据大于64K，就必须使用到mediumtext ，longtext，因为varchar已经存不下了
- varchar（255+）之后，和text在存储机制是一样的，性能也相差无几（InnoDB默认只会存放前768字节在数据页中，而剩余的数据则会存储在溢出段中）

> InnoDB引擎单一字段索引的默认长度最大为767字节，MyISAM为1000字节。超出限制会导致索引创建不成功，转而需要创建前缀索引
>
> https://www.h5w3.com/118004.html
>
> https://blog.csdn.net/weixin_33697898/article/details/92295388 很重要，虽然有点乱

# 十七、Mysql死锁

**一、什么是死锁**

官方定义如下：两个事务都持有对方需要的锁，并且在等待对方释放，并且双方都不会释放自己的锁。

![Mysql死锁](http://longls777.oss-cn-beijing.aliyuncs.com/img/Mysql死锁.png)

**二、为什么会形成死锁**

MySQL的并发控制有两种方式，一个是 MVCC，一个是两阶段锁协议。那么为什么要并发控制呢？是因为多个用户同时操作 MySQL 的时候，为了提高并发性能并且要求如同多个用户的请求过来之后如同串行执行的一样（可串行化调度）

**两阶段锁协议（2PL）**

官方定义：

两阶段锁协议是指所有事务必须分两个阶段对数据加锁和解锁，在对任何数据进行读、写操作之前，事务首先要获得对该数据的封锁；在释放一个封锁之后，事务不再申请和获得任何其他封锁。

对应到 MySQL 上分为两个阶段：

1. 扩展阶段（事务开始后，commit 之前）：获取锁
2. 收缩阶段（commit 之后）：释放锁

就是说呢，只有遵循两段锁协议，才能实现可串行化调度。

但是两阶段锁协议不要求事务必须一次将所有需要使用的数据加锁，并且在加锁阶段没有顺序要求，所以这种并发控制方式会形成死锁。

**三、MySQL 如何处理死锁？**

MySQL有两种死锁处理方式：

1. 等待，直到超时（innodb_lock_wait_timeout=50s）
2. 发起死锁检测，主动回滚一条事务，让其他事务继续执行（innodb_deadlock_detect=on）

由于性能原因，一般都是使用死锁检测来进行处理死锁。

**死锁检测**

死锁检测的原理是构建一个以事务为顶点、锁为边的有向图，判断有向图是否存在环，存在即有死锁

**回滚**

检测到死锁之后，选择插入更新或者删除的行数最少的事务回滚，基于 INFORMATION_SCHEMA.INNODB_TRX 表中的 trx_weight 字段来判断。

**四、如何避免发生死锁**

**收集死锁信息：**

1. 利用命令 SHOW ENGINE INNODB STATUS查看死锁原因
2. 调试阶段开启 innodb_print_all_deadlocks，收集所有死锁日志

**减少死锁：**

1. 使用事务，不使用 lock tables 
2. 保证没有长事务。
3. 操作完之后立即提交事务，特别是在交互式命令行中
4. 如果在用 (SELECT ... FOR UPDATE or SELECT ... LOCK IN SHARE MODE)，尝试降低隔离级别
5. 修改多个表或者多个行的时候，将修改的顺序保持一致
6. 创建索引，可以使创建的锁更少。
7. 最好不要用 (SELECT ... FOR UPDATE or SELECT ... LOCK IN SHARE MODE)
8. 如果上述都无法解决问题，那么尝试使用 lock tables t1, t2, t3 锁多张表