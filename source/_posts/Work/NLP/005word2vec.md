---
title: word2vec
tags: 八股
categories: NLP
date: 2023-08-02 13:06:30
index_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-35339b4e3efc29326bad70728e2f469c_1440w.webp
banner_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-35339b4e3efc29326bad70728e2f469c_1440w.webp
math: true
---

> 理解 Word2Vec 之 Skip-Gram 模型 - 天雨粟的文章 - 知乎 https://zhuanlan.zhihu.com/p/27234078
>
> word2vec详解（CBOW，skip-gram，负采样，分层Softmax） - 孙孙的文章 - 知乎 https://zhuanlan.zhihu.com/p/53425736、
>
> Hierarchical Softmax（层次Softmax） - 清欢鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/56139075
>
> 优化技巧：负采样和分层softmax - 热血老男孩的文章 - 知乎 https://zhuanlan.zhihu.com/p/568064512

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-35339b4e3efc29326bad70728e2f469c_1440w.webp)

## 一、什么是Word2Vec和Embeddings？

Word2Vec 是一种用于将单词表示为连续向量空间中的向量的技术。它是一种词嵌入（Word Embedding）方法，用于将文本中的单词映射到实数向量，使得具有相似语义的单词在向量空间中距离较近。

它有两种不同的模型：CBOW（Continuous Bag of Words）和Skip-gram

- CBOW：CBOW模型的目标是从上下文中的周围单词预测当前单词，在已知 `context(w)` 的情况下，预测 `w`
- Skip-gram：Skip-gram 模型的目标是从当前单词预测上下文中的周围单词，在已知 `w` 的情况下预测 `context(w)`

Word2Vec的优点是，它可以将离散的符号化单词转换为实数向量，从而使得文本数据可以在连续向量空间中进行计算和表示。 

Embeddings（嵌入）是一种表示学习的技术，用于将高维的离散数据映射到低维的连续向量空间。Word2Vec 就是一种词嵌入方法，它用于将单词表示为低维的向量，使得单词的语义和语法关系在向量空间中得到保留。



## 二、说说CBOW？

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-73885c25dd5d7cbbba9e7834b18c0aa3_1440w.webp)



- CBOW也就是连续词袋模型，目标是从上下文中的周围单词预测当前单词，最大化给定上下文时中心单词出现的概率，也就是最小化交叉熵损失函数
- 参数包括输入层到隐藏层之间的权重矩阵W（大小为V×N），以及隐藏层到输出层之间的权重矩阵U（大小为N×V）
- 训练完成后，我们可以将W或者U作为生成的词向量矩阵。一般来说，W比U更常用
- 平均池化：在CBOW模型中，上下文单词的向量通过求平均值来得到上下文表示。具体地，模型将上下文单词的向量进行平均，得到一个上下文表示

**CBOW模型的优缺点**

CBOW模型相比于传统的基于计数或者基于矩阵分解等方法生成词向量有以下优点：

- 能够利用大规模语料库进行训练
- 能够学习到高质量且低维度（通常在50~300之间） 的稠密向量，节省存储空间和计算资源
- 能够捕捉单词之间的复杂关系，如同义词、反义词、类比关系等

CBOW模型也有以下缺点：

- 由于平均池化操作的存在，忽略了上下文单词的顺序，只考虑了它们的累加效果，因此可能对长文本序列中的语义关系把握不如Skip-gram模型准确

- 对于低频或者生僻单词，可能无法生成准确的词向量

  

## 三、说说Skip-gram？

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-0ed8b9efc8aa7867fbf001b921cec4ee_1440w.webp)

- Skip-gram 模型的目标是从当前单词预测上下文中的周围单词
- Skip-gram 模型的输出层是一个 softmax 分类器，它将当前单词的向量作为输入，预测上下文单词的向量

**Skip-gram模型的优缺点**

Skip-gram 模型的优点是：

- 能够捕捉单词之间的顺序信息，因为它通过预测上下文单词来学习单词的表示。相比之下，CBOW 模型通过平均池化操作无法捕捉单词之间的顺序信息

Skip-gram 模型的缺点是：

- 训练速度相对较慢，因为它需要预测每个上下文单词，导致计算复杂度较高



## 四、说说负采样？

负采样（Negative Sampling）是 Word2Vec 模型中的一种训练技巧，用于加速模型的训练过程，并且在一定程度上解决了原始的 Softmax 计算复杂度过高的问题

在 Word2Vec 模型中，当词汇表较大时，softmax 计算的复杂度会随之增加，导致训练速度变慢，尤其是在大规模文本数据上

负采样通过一种近似的方式来替代原始的 softmax 函数，从而减少计算复杂度。具体来说，对于 Skip-gram 或 CBOW 模型的每个训练样本（一个当前单词和其上下文单词），负采样从词汇表中随机采样若干个负样本（即非上下文单词），将这些负样本视为错误的预测目标。然后，模型只需要计算当前单词和上下文单词以及少量负样本之间的二分类概率，来判断哪些单词应该是上下文单词，哪些单词应该是负样本

- 负采样的优点：
  - 加速训练过程
  - 提高效率
- 负采样的缺点：
  - 由于随机采样负样本，负采样可能导致一些负样本与上下文实际上具有一定的语义关系，从而影响模型的性能



## 五、说说层次softmax？

层次 softmax（Hierarchical Softmax）是一种用于解决 Word2Vec 中 softmax 计算复杂度过高的问题的技术。它通过构建一个二叉树来代替原始的平坦的词汇表，从而减少了 softmax 计算的复杂度

在 Word2Vec 中，softmax 函数用于计算当前单词或上下文单词在整个词汇表中的概率分布。对于一个词汇表大小为 V 的问题，softmax 的计算复杂度是 O(V)，因为需要计算所有单词的概率分布。这在大规模文本数据上会导致计算非常耗时。

层次 softmax 通过使用二叉树来组织词汇表，将词汇表划分为一系列的层次结构。在这个二叉树中，每个叶子节点都代表一个单词，而非叶子节点则代表一个虚拟的词。每个非叶子节点都有两个子节点，分别代表在给定上下文条件下的“是”和“否”的概率。在训练过程中，模型只需沿着二叉树的路径进行传递，并计算沿途节点的概率，而不需要计算整个词汇表的概率分布。这样可以大大降低 softmax 的计算复杂度。

具体来说，给定一个上下文单词和当前单词，层次 softmax 通过从根节点开始，沿着二叉树的路径向下，依次计算沿途节点的概率。在每个节点处，模型需要判断当前单词应该是“是”还是“否”，然后根据对应的概率进行下一步的选择。最终，模型将沿途节点的概率乘积作为当前单词的预测概率。

- 层次 softmax 的优点：
  - 显著降低了 softmax 计算的复杂度，特别是在大规模词汇表上，计算速度得到了显著提升。

- 层次 softmax 的缺点：
  - 构建和维护二叉树需要额外的存储空间和计算资源
  - 对于低频词或较少出现的词，层次 softmax 可能会有较差的表现，因为这些词在二叉树中可能位于较深的位置

