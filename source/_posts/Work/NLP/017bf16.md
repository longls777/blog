---
title: BF16和FP16的区别
tags: 
categories: NLP
date: 2023-8-31 13:37:00
index_img: https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230831134249416.png
banner_img: 
math: true
---



> 为什么很多新发布的LLM模型默认不用float16呢？ - DMRio3Uziy5c90dy的回答 - 知乎 https://www.zhihu.com/question/616600181/answer/3160547952

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/v2-b5dedadc752de827fa74cb8ee4cbd5e8_1440w.webp)

bfloat16 和 float32 浮点数都拥有 8 位指数位，而 float16 仅拥有 5 个指数位，这能使得 bfloat16 相比 float16 表示更加大的范围

```python
>> torch.finfo(torch.float32)
>> finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)
>> torch.finfo(torch.float16)
>> finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)
>> torch.finfo(torch.bfloat16)
>> finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)
```

其中，`min` `max` 字段为该数据类型可表示的最大最小值。 

`tiny` 和 `smallest_normal` 含义相同，表示该数据类型可以表示的最小正数。

`eps` 字段为该数据类型的最小可表示数，满足 `1.0 + eps != 1.0` 。

`resolution` 数据类型对应的近似十进制精度。 

可以看到，虽然精度比 float16 要差很多，但 bfloat16 数据类型拥有和 float32 相同的表示范围。

这一特性在现代 LLM 学习任务中是更重要的，由于自注意力机制中使用了大量的 exp 运算，因此很多情况下，更大的范围比精度重要很多（能够有效防止上下溢出）