---
title: 逻辑回归
tags: 
categories: Machine Learning
date: 2023-7-20 19:45:00
index_img:
banner_img:
math: true
---

> https://blog.csdn.net/perfect1t/article/details/82966647

逻辑回归主要用于二分类，其主要思想是：**根据现有数据对分类边界线建立回归公式，以此进行分类**

将线性回归$z = w^Tx + b$ 输入到sigmoid函数$y = \frac{1}{1+e^{-z}}$：
$$
y = \frac{1}{1 + e^{-(w^Tx+b)}}
$$
从而得到：
$$
ln\frac{y}{1-y} = w^Tx+b
$$
$y$为正例的概率，$1-y$为负例的概率，则$\frac{y}{1-y}$称为几率，反映了作为正例的相对可能性。对几率取对数称为对数几率（logit）

**实际上在利用线性回归模型的预测结果去逼近真实label的对数几率**

****

#### 使用最大似然估计求解：

> 对数损失函数（logLoss，也就是二元交叉熵损失）

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230720200615852.png)

最小化该损失函数即可



## 另一种推导

![LR推导](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731105656184.png)



## 为什么常常要做特征组合（特征交叉）？

LR 模型属于线性模型，线性模型不能很好处理非线性特征，**特征组合可以引入非线性特征**，提升模型的表达能力。 另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏， 对每一个样本建模又会导致数据爆炸，过拟合，所以基本特征+特征组合兼顾了全局和个性化。



## LR 与最大熵模型 MaxEnt 的关系?

没有本质区别。LR 是最大熵对应类别为二类时的特殊情况，也就是当LR 类别扩展到多类别时，就是最大熵模型



## 为什么 LR 用交叉熵损失而不是平方损失（MSE）？

> 为什么分类问题的损失函数采用交叉熵而不是均方误差MSE？ - 萌水水的文章 - 知乎 https://zhuanlan.zhihu.com/p/104130889

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731111630430.png)

#### 另一种解释

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731110901609.png)

