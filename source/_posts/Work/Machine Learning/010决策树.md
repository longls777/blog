---
title: 决策树
tags: 
categories: Machine Learning
date: 2023-7-31 14:51:00
index_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/c344d332039f42cbb601bdfe19af1ebfc393872aef2a460c86efe4c8dc0d10db
banner_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/c344d332039f42cbb601bdfe19af1ebfc393872aef2a460c86efe4c8dc0d10db
math: true
---



> https://aistudio.baidu.com/aistudio/projectdetail/1700924?ad-from=1636
>
> 机器学习—决策树详解 - Cheers的文章 - 知乎 https://zhuanlan.zhihu.com/p/537933555
>
> https://blog.csdn.net/weixin_44225602/article/details/106909383



## 定义

决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果



## ID3算法

ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。 其大致步骤为：

1. 初始化特征集合和数据集合；
2. 计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
3. 更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点



ID3 使用的分类标准是**信息增益**，它表示**得知特征 A 的信息而使得样本集合不确定性减少的程度**。数据集的信息熵公式如下：
$$
H(D)=-\sum_{k=1}^{K}{\frac{|C_k|}{|D|}log_2{\frac{|C_k|}{|D|}}}
$$
其中$C_k$表示集合D中属于第k类样本的样本子集

针对某个特征A，对于数据集D的条件熵$H(D|A)$为：
$$
\begin{aligned}
H(D \mid A) & =\sum_{i=1}^n \frac{\left|D_i\right|}{|D|} H\left(D_i\right) \\
& =-\sum_{i=1}^n \frac{\left|D_i\right|}{|D|}\left(\sum_{k=1}^K \frac{\left|D_{i k}\right|}{\left|D_i\right|} \log _2 \frac{\left|D_{i k}\right|}{\left|D_i\right|}\right)
\end{aligned}
$$
其中$D_i$表示D中特征A取第$i$个值的样本子集，$D_{ik}$表示$D_i$中属于第k类的样本子集

**信息增益 = 信息熵 - 条件熵**
$$
Gain(D,A) = H(D) - H(D|A)
$$
信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”



**缺点：**

- ID3 没有剪枝策略，容易过拟合；
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
- 只能用于处理离散分布的特征；
- 没有考虑缺失值



## C4.5算法

C4.5 算法最大的特点是克服了 **ID3 对特征数目的偏重**这一缺点，引入信息增益率来作为分类标准

C4.5 相对于 ID3 的缺点对应有以下改进方式：

1. 引入悲观剪枝策略进行后剪枝；
2. 引入信息增益率作为划分标准；
3. 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择**信息增益最大的点**作为该连续特征的二元离散分类点；

#### 2.2 对于缺失值的处理可以分为两个子问题

问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）

C4.5 的做法是：**对于具有缺失值特征，用没有缺失的样本子集所占比重来折算**



问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）

C4.5 的做法是：**将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中**



利用信息增益率可以克服信息增益的缺点，其公式如下：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/ac9ea9924609411da90bb66b62c47e86c49d201e4931426e8411cf5d3c17d4ca)

$H_A(D)$称为特征A的固有值

> 这里需要注意，信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个启发式方法：**先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的**

#### 剪枝策略

为什么要剪枝？

过拟合的树的泛化能力非常差

- 预剪枝
  - 在节点划分前来确定是否继续增长，及早停止增长的主要方法有：
    - 节点内数据样本低于某一阈值
    - 所有节点特征都已分裂
    - 节点划分前准确率比划分后准确率高
    - 预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险

- 后剪枝
  - 在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树
  - C4.5 采用的悲观剪枝方法，**用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。**C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率
  - 后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但同时其训练时间会大的多

**缺点**

- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行



## CART算法

ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率



**CART 包含的基本过程有分裂，剪枝和树选择 ：**

- 分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；
- 剪枝：采用**代价复杂度剪枝**，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；
- 树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）



 **CART 在 C4.5 的基础上进行了很多提升：**

- C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；
- C4.5 只能分类，CART 既可以分类也可以回归；
- CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；
- CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；
- CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法



熵模型拥有大量耗时的对数运算，**基尼指数在简化模型的同时还保留了熵模型的优点**。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/c9937bfc922546d3bd17cd7892e4c34e3616570e53934a6e9aab7654251123cc)

**基尼指数反映了从数据集中随机抽取两个样本，其类别标记不一致的概率**。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，当 CART 为二分类，其表达式为：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/213cbbfdea164e4e9be026cdf62dda9a82148e56596545b7a47370a266aa8553)



在平方运算和二分类的情况下，其运算更加简单。当然其性能也与熵模型非常接近。那么问题来了：基尼指数与熵模型性能接近，但到底与熵模型的差距有多大呢？我们知道$ln(x) = -1 + x + o(x)$，所以：

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/7658f96b803b46249b3bcd77bfbc57b5a9569806ff3a45e8aa2df1df6bdb9c3a)

基尼指数可以理解为熵模型的一阶泰勒展开



![](http://longls777.oss-cn-beijing.aliyuncs.com/img/e2e1e64b166a4c4f87a703d220b6f8f149d4ab914c1b4859b92c8650e68243bd)



**缺失值处理**

问题一：如何在特征值缺失的情况下进行划分特征的选择？

CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）



问题二：选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？

CART 算法的机制是为树的每个节点都找到**代理分裂器**，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，**特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征）**，当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含缺失值的新数据



**剪枝策略**

采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点。然后用一种**成本复杂度的度量准则**来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用**一个单独的测试数据集**来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。

- 首先我们将最大树称为$T_0$，我们希望减少树的大小来防止过拟合，但又担心去掉节点后预测误差会增大，所以我们定义了一个损失函数来达到这两个变量之间的平衡。损失函数定义如下：
  $$
  C_{\alpha}(T) = C(T) + \alpha|T|
  $$

- $T$为任意子树，$C(T)$为预测误差，$|T|$为子树$T$的叶子节点个数，$\alpha$是参数， $C(T)$衡量训练数据的拟合程度， $∣T∣$衡量树的复杂度，$\alpha$权衡拟合程度与树的复杂度

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731155202133.png)



**类别不平衡**

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731155500550.png)



**回归树**

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731155702026.png)



## 为什么 C4.5 使用信息增益比？

在使用信息增益的时候，如果某个特征有很多取值，使用这个取值多的特征会的大的信息增益，这个问题是出现很多分支，将数据划分更细，模型复杂度高，出现过拟合的机率更大。使用信息增益比就是为了解决偏向于选择取值较多的特征的问题。使用信息增益比对取值多的特征加上的惩罚，对这个问题进行了校正

​	

## 基尼指数和信息熵都表示数据不确定性，为什么CART 使用基尼指数？

无论是ID3还是C4.5，都是基于信息论的熵模型的，这里面会涉及大量的对数运算

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-3525b5e1845637ed3185e89fb597ca4e_1440w.webp)

而CART相当于对熵公式在x=1进行一阶泰勒展开，近似于半熵，同时也能减少计算量

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230731160514752.png)

> cart算法为什么选用gini指数？ - tmac的回答 - 知乎 https://www.zhihu.com/question/36659925/answer/223255104



## 基尼系数（Gini）存在的问题?

基尼系数对于类别较大的数据集更加偏向于选择大类别作为划分特征。因为基尼系数的计算是对类别概率的平方和，这导致它对于具有较大类别的数据更敏感，可能会导致决策树在划分时偏向于选择大类别，而忽略了少数类别的重要性。