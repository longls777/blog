---
title: 集成学习
tags: 
categories: Machine Learning
date: 2023-8-1 14:26:00
index_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-9000c0e50e1a97d0d12e85dc93affa5f_720w.jpg
banner_img: http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-9000c0e50e1a97d0d12e85dc93affa5f_720w.jpg
math: true
---

> 集成学习（Ensemble Learning) - Will的文章 - 知乎 https://zhuanlan.zhihu.com/p/27689464
>
> https://blog.csdn.net/qq_38147421/article/details/120019421

![集成学习（Ensemble Learning)](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-9000c0e50e1a97d0d12e85dc93affa5f_720w.jpg)

在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。

> 通过结合多个学习器(例如同种算法但是参数不同，或者不同算法)，一般会获得比任意单个学习器都要好的性能，尤其是在这些学习器都是"弱学习器"的时候提升效果会很明显。



## Bagging

Bagging是bootstrap aggregating的简写。bootstrap也称为自助法，它是一种**有放回的抽样方法**，目的为了得到统计量的分布以及置信区间，具体步骤如下：

- **采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本**
- **根据抽出的样本计算想要得到的统计量T**
- **重复上述N次（一般大于1000），得到N个统计量T**
- **根据这N个统计量，即可计算出统计量的置信区间**

![Bagging](http://longls777.oss-cn-beijing.aliyuncs.com/img/baf66c5b70b64a65bb694aced40df1fa.jpg)

在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。

例如**随机森林（Random Forest）**就属于Bagging。随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。

在我们学习每一棵决策树的时候就需要用到Bootstrap方法。在随机森林中，有两个随机采样的过程：对输入数据的**行（数据的数量）与列（数据的特征）**都进行采样。对于行采样，采用有放回的方式，若有N个数据，则采样出N个数据（可能有重复），这样在训练的时候每一棵树都不是全部的样本，相对而言不容易出现overfitting；接着进行列采样从M个feature中选择出m个（m<<M）

预测的时候，随机森林中的每一棵树的都对输入进行预测，最后进行投票，哪个类别多，输入样本就属于哪个类别。这就相当于前面说的，每一个分类器（每一棵树）都比较弱，但组合到一起（投票）就比较强了



## Boosting

**提升方法（Boosting）**是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是**AdaBoost（Adaptive boosting）算法**：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。

![Boosting](http://longls777.oss-cn-beijing.aliyuncs.com/img/c552d824c80a4b6caa4e0b34bc64aba8.png)



### GBDT（Gradient Boost Decision Tree，梯度提升决策树）

> Gradient Boosting - Will的文章 - 知乎 https://zhuanlan.zhihu.com/p/26327929



GBDT 每一次的计算是都为了减少上一次的残差，进而在残差减少（负梯度）的方向上建立一个新的模型

![GBDT](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801144541685.png)



#### GBDT 训练过程如何选择特征？

GBDT 使用基学习器是 CART 树，CART 树是二叉树，每次使用yes or no 进行特征选择，数值连续特征使用的最小均方误差，离散值使用的gini 指数。在每次划分特征的时候会遍历所有可能的划分点找到最有的特征分裂点，这是用为什么gbdt 会比 rf（随机森林） 慢的主要原因之一



#### GBDT 如何防止过拟合？由于GBDT是前向加法模型，前面的树往往起到决定性的作用，如何改进这个问题？

一般使用缩减因子对每棵树进行降权，可以使用带有 dropout 的GBDT 算法，dart 树，随机丢弃生成的决策树，然后再从剩下的决策树集中迭代优化提升树。



#### GBDT 对标量特征要不要 one-hot 编码？

从效果的角度来讲，使用category特征和one-hot是等价的，所不同的是category特征的feature空间更小。微软在lightGBM的文档里也说了，category特征可以直接输入，不需要one-hot编码，准确度差不多，速度快8倍，而sklearn的tree方法在接口上不支持category输入，所以只能用one-hot编码



#### 为什么 GBDT 用负梯度当做残差？

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801145227874.png)

- 负梯度方向可证，往这个方向优化模型一定会收敛
- 如果想用残差为负梯度，这里损失函数一般要用均方损失，但是并不是所有的问题都可以用均方损失，如分类问题就不可以
- **对于一些损失函数来说最大的残差方向，并不是梯度下降最好的方向**，损失函数最小与残差最小两者目标不统一



### AdaBoost（自适应提升）

> AdaBoost概述 - ARGO创新实验室的文章 - 知乎 https://zhuanlan.zhihu.com/p/153011824



![AdaBoost](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-b629af21db21693e3ff571e122e0e066_1440w.webp)

Adaboost 既可以用作分类，也可以用作回归

![AdaBoost算法流程](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801150650106.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801150758544.png)

#### 为什么 Adaboost 方式能够提高整体模型的学习精度？

根据前向分布加法模型，Adaboost 算法每一次都会降低整体的误差，虽然单个模型误差会有波动，但是整体的误差却在降低，整体模型复杂度在提高



#### 使用m个基学习器和加权平均使用m个学习器之间有什么不同？

Adaboost 的 m 个基学习器是有顺序关系的，第 k 个基学习器根据前 k - 1 个学习器得到的误差更新数据分布，再进行学习，每一次的数据分布都不同，是使用同一个学习器在不同的数据分布上进行学习。

 加权平均的 m 个学习器是可以并行处理的，在同一个数据分布上，学习得到 m 个不同的学习器进行加权。



#### Adaboost的迭代次数（基学习器的个数）如何控制？

一般使用 earlystopping 进行控制迭代次数



## XGBoost（eXtreme Gradient Boosting，极端梯度提升）

> https://blog.csdn.net/qq_38147421/article/details/120019421



基于 boosting 增强策略的加法模型，训练的时候采用前向分布算法进行贪婪学习，每次迭代都学习一棵 CART 树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。

 XGBoost 对 GBDT 进行了一系列优化，比如**损失函数进行了二阶泰勒展开**、**目标函数加入正则项**、**支持并行和默认缺失值处理**等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。



#### XGBoost 使用泰勒二阶展开的原因？

- 精准性：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数 
- 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导



#### XGBoost的目标函数和残差之间的关系是什么？

简单来说，目标函数中的损失函数的一阶导数为残差**（负梯度）**

xgboost的目标是为了让**前面拟合的模型+下一颗树之后的预测值—>真实值**。因此目标函数即为损失函数（降低偏差）+ 正则项（降低方差，避免过拟合），并且希望值最小。我们所要求的其实就是每一步损失函数的一阶导和二阶导的值，就可以然后最优化目标函数，从而最后根据加法模型得到一个整体模型。



#### XGBoost 可以并行训练的原因？

XGBoost 的并行，并不是说每棵树可以并行训练，XGB 本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。

XGBoost 的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为 Block 结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个 block 结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个 block 并行计算。



#### XGBoost 为什么快？

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801152144726.png)



#### XGBoost 中叶子结点的权重如何计算出来？

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230801152452035.png)



#### XGBoost 中的一棵树的停止生长条件？

- 当新引入的一次分裂所带来的增益 Gain<0 时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程
- 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数 max_depth
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细



## Stacking

Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。

如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，称之为Tier 1分类器（可以采用交叉验证的方式学习），然后将输出用于训练Tier 2 分类器。

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/v2-bc3b2612dd0ff778c53db4165bc35449_1440w.webp)

