---
title: Matching the Blanks - Distributional Similarity for Relation Learning
tags: RE
categories: Paper Reading Notes
date: 2023-09-02 13:45:00
index_img: https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902125205979.png
banner_img: 
math: true
---

**标题：**《Matching the Blanks: Distributional Similarity for Relation Learning》

**论文来源：** ACL  2019

**原文链接： **  https://aclanthology.org/P19-1279/

**源码：** https://github.com/plkmo/BERT-Relation-Extraction



## 概述

基于对Harris的分布假设进行的扩展，以及在学习文本表示方面的进展（具体来说，是BERT），仅从与实体相关联的文本构建了与任务无关的关系表示

> (来自ChatGPT)
>
> Harris' distributional hypothesis是一种语言学假设，最早由美国语言学家Zellig Harris于1954年提出。这一假设的核心思想是：
>
> "Words that occur in the same contexts tend to have similar meanings."
>
> 也就是说，如果两个词在语境中出现的模式（即它们周围的其他词汇）相似，那么这两个词可能具有相似的含义或关联性。这一假设强调了词汇的语境和共现关系在理解词义和语言中的作用。根据这一观点，我们可以通过分析词汇在大量文本中的共现模式来推断它们之间的语义关系。
>
> Harris' distributional hypothesis在自然语言处理和计算语言学领域中具有重要影响，因为它为词嵌入（word embeddings）和语言模型的发展提供了理论基础。例如，Word2Vec和GloVe等词嵌入模型就是基于这一假设的思想构建的，它们通过分析词汇在大规模语料库中的共现来生成词向量，以捕捉词汇之间的语义相似性。



![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902124125243.png)

## Method

### Relation Classification and Extraction Tasks

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902125205979.png)

上图是使用的两个Loss，左边是监督学习分类loss，右边是用于训练few-shot能力的相似度loss

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902125408063.png)

这里有6种Encoder的输入输出变体，它们的结果如下：

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902125811635.png)

可以看到使用Entity Markers（[E1] [/E1] [E2] [/E2]）并且提取实体开头的Markers（[E1] [E2]）作为关系表征效果最好

###  Learning by Matching the Blanks

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902131715740.png)

通过最小化下面的Loss来学习$f_{\theta}$

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902131815843.png)

这个Loss很像逻辑回归Loss呀

> [损失函数总结](http://lishilong.xyz/2023/07/20/Deep%20Learning/005%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/)

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902132640904.png)

其实就是上图这样，必须两个实体都一样才是正例，否则都是反例



![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902132949914.png)

这里就是以概率$\alpha$随机替换entity为[BLANK]，从而构建新的数据集，目的是让$f_{\theta}$学会entity span

### Matching the Blanks Training

训练的时候，类似BERT，使用两个Loss：

- the masked language model loss
- the matching the blanks loss

从Wikipedia提取文本，并使用Google Cloud Natural Language API 标记实体（NER）

> The span annotations include **not only proper names**, but other referential entities such as common **nouns** and **pronouns**. From this annotated corpus we extract relation statements where each statement contains **at least two grounded entities** within **a fixed sized window** of tokens. To prevent a large bias towards relation statements that involve popular entities, we limit the number of relation statements that contain the same entity by randomly sampling a constant number of relation statements that contain any given entity.

此外还有一些实现细节：由于无法比较每对关系陈述，采用了噪声-对比估计方法。考虑所有包含相同实体的正对关系陈述对，而对于不包含相同实体对的关系陈述对，进行了随机采样或从共享一个实体的关系陈述中采样。还以一定概率将实体的提及替换为[BLANK]符号，以确保模型不会受到[BLANK]符号缺失的干扰

最终生成了600m个关系陈述对，其中约50%为正对，50%为强负对



## Experiments

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902133948579.png)



![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902134237692.png)



![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902134314867.png)

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230902134329630.png)

