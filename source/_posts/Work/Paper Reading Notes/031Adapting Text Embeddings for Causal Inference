---
title: Adapting Text Embeddings for Causal Inference
tags: Causal Inference
categories: Paper Reading Notes
date: 2023-09-03 21:50:00
index_img: https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903215419961.png
banner_img: 
math: true
---



**标题：**《Adapting Text Embeddings for Causal Inference》

**论文来源：** UAI 2020

**原文链接： **  https://arxiv.org/abs/1905.12741

**源码：** https://github.com/vveitch/causal-text-embeddings-tf2



## 概述

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903215419961.png)

这里假设文本$W$是混淆因素，它本身具有足够进行因果调整的信息，但是维度太高了（所以不能因果调整？）

所以本文主要目的之一是给文本降维，去除冗余信息，只保留足够进行因果调整的信息

- 监督降维：因果调整只需要文本中对治疗和结果都有预测性的方面
- 高效语言建模：文本的表示被设计用来排除与语言无关的信息，而这些信息在因果上也是无关的

## Method

首先是ATT，使用do算子固定T

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903215852532.png)

本文假设，$W$有足够的信息来调整$T$和$Y$之间的混杂，因此本文的目标是找到一个$Z=f(W)$，来关闭$T$和$Y$之间的所有后门路径：

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903220033951.png)

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903220245590.png)

看起来倾向得分$g(z)$还是通过结果$T$划分之后的概率

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903220356726.png)

下面是为什么高维的embedding不能直接估计因果效应的解释：high-variance estimator

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903220642031.png)

解决方法是从单词$W$学习一个embedding vector来捕获混淆

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903220809053.png)

> RR83 https://arxiv.org/pdf/1802.02163.pdf

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903221144629.png)

这个$\lambda$类似于正常训练loss下生成的的降维矩阵呀

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903221350141.png)

![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903221529476.png)



![](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20230903222145548.png)

看不懂了！！！！！！
