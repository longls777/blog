---
title: A unified perspective of RLHF
tags: LLM RLHF
categories: Work
date: 2024-10-29 15:49:00
index_img: https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20241030133028665.png
banner_img: https://longls777.oss-cn-beijing.aliyuncs.com/img/pexels-sergey-pesterev-69811391-14578422.jpg
math: true
---

# Currently popular RLHF Method

![rlhf pipline](https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20241030133028665.png)

## PPO

## DPO

## ORPO

## KTO

## IPO

## SimPO



# Relate Work

## UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function

**paper**: https://arxiv.org/pdf/2408.15339

**source**: Salesforce























> [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
>
> [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
>
> https://lilianweng.github.io/posts/2018-04-08-policy-gradient/
>
> [关于LLM+RL(HF)的片面脉络梳理](https://zhuanlan.zhihu.com/p/1686790674)