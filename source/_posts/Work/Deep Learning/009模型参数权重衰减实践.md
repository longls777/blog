---
title: 模型参数权重衰减实践
tags: 
categories: Deep Learning
date: 2023-7-22 17:57:00
index_img:
banner_img:
math: true
---



在阅读minGPT代码时，发现了控制权重衰减的一个操作，具体代码如下：



```python
def configure_optimizers(self, train_config):
        """
        This long function is unfortunately doing something very simple and is being very defensive:
        We are separating out all parameters of the model into two buckets: those that will experience
        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).
        We are then returning the PyTorch optimizer object.
        """

        # separate out all parameters to those that will and won't experience regularizing weight decay
        decay = set()
        no_decay = set()
        whitelist_weight_modules = (torch.nn.Linear, )
        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
        for mn, m in self.named_modules():
            for pn, p in m.named_parameters():
                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name
                # random note: because named_modules and named_parameters are recursive
                # we will see the same tensors p many many times. but doing it this way
                # allows us to know which parent module any tensor p belongs to...
                if pn.endswith('bias'):
                    # all biases will not be decayed
                    no_decay.add(fpn)
                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
                    # weights of whitelist modules will be weight decayed
                    decay.add(fpn)
                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
                    # weights of blacklist modules will NOT be weight decayed
                    no_decay.add(fpn)

        # validate that we considered every parameter
        param_dict = {pn: p for pn, p in self.named_parameters()}
        inter_params = decay & no_decay
        union_params = decay | no_decay
        assert len(inter_params) == 0, "parameters %s made it into both decay/no_decay sets!" % (str(inter_params), )
        assert len(param_dict.keys() - union_params) == 0, "parameters %s were not separated into either decay/no_decay set!" \
                                                    % (str(param_dict.keys() - union_params), )

        # create the pytorch optimizer object
        optim_groups = [
            {"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": train_config.weight_decay},
            {"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
        ]
        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)
        return optimizer
```



**其中bias都不进行权重衰减，weight只对torch.nn.Linear进行权重衰减，而不对torch.nn.LayerNorm和torch.nn.Embedding进行，原因是什么呢？**



> 以下内容来自chatGPT

## 为什么不对bias进行权重衰减

在深度学习中，权重衰减（Weight Decay）是一种常用的正则化技术，用于防止模型过拟合。它通过在优化过程中对权重进行惩罚，即在损失函数中加入权重的平方范数作为正则化项，以限制模型权重的大小。

**对于神经网络中的全连接层或卷积层，通常将权重衰减应用于权重参数，因为这些参数对模型的输出有直接影响，并且对其进行正则化可以提高模型的泛化能力。**

**然而，对于偏置项（bias），它通常被认为是一种模型的偏移量，不像权重参数那样直接影响模型输出。因为偏置项并不涉及输入的乘法运算，它只会对输出进行平移操作，所以不会像权重参数那样对模型过拟合产生显著的影响。**

因此，在实践中，通常将权重衰减应用于权重参数，而不对偏置项进行权重衰减。**这样可以减少计算开销，同时也能达到较好的正则化效果。**



## 为什么只对torch.nn.Linear的weight进行权重衰减

主要原因有两个：

1. **归一化层的参数对模型的过拟合影响较小**：归一化层（如LayerNorm）的参数通常是用来调整输入特征的均值和方差，对模型的过拟合影响较小。这是因为归一化操作是对每个样本独立进行的，并且其参数的数量通常较少。因此，对归一化层的参数进行权重衰减一般不会带来明显的正则化效果。
2. **权重衰减对模型的影响主要集中在权重参数上**：权重衰减通过在损失函数中添加权重的平方范数作为正则化项，限制权重的大小。权重参数控制着神经网络的连接强度和特征的权重，对模型的输出产生直接影响。因此，对权重参数进行权重衰减可以有效地控制模型的复杂度，防止过拟合。

综上所述，通常在深度学习中，对于权重衰减技术，只对权重参数（如全连接层的权重）进行权重衰减，而不对偏置项和归一化层的参数（如LayerNorm和Embedding）进行权重衰减。这是为了保持模型的有效性和计算效率，同时有效地控制模型的复杂度和减少过拟合的风险。



## 为什么不对torch.nn.Embedding进行权重衰减

1. 嵌入层的参数是离散的整数索引到连续的向量映射：嵌入层的参数是词向量或字符向量，它们对应于整数索引（例如单词或字符的ID）到连续的向量表示的映射。这些向量在训练过程中是从数据中学习得到的，而不是通过模型的优化过程得到的。因此，对嵌入层的参数进行权重衰减可能并不适用，**因为它们不是通过模型优化得到的典型的线性权重参数**。
2. **嵌入层通常拥有较少的参数**：嵌入层的参数个数通常远远小于其他层（例如全连接层）的参数个数。对于大规模的词汇表，嵌入层的参数可能有几十万或几百万，但相对于整个模型的参数数量来说，它们仍然是相对较少的。因此，即使对嵌入层的参数进行权重衰减，对整个模型的影响可能也是有限的。

综上所述，由于嵌入层的特殊性质和相对较少的参数数量，通常不会对 `torch.nn.Embedding` 进行权重衰减。相反，权重衰减通常应用于其他具有更多参数和线性权重的层，例如全连接层。